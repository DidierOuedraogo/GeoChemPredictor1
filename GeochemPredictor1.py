import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from sklearn.ensemble import IsolationForest, RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, DBSCAN
from sklearn.neighbors import KNeighborsRegressor
import xgboost as xgb
import lightgbm as lgb
from scipy.interpolate import griddata
from scipy.spatial import Voronoi, voronoi_plot_2d
import matplotlib.pyplot as plt
from io import BytesIO
import base64
import time
import uuid

# Configuration de base de l'application
st.set_page_config(
    page_title="GeoChem Predictor",
    page_icon="üåã",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Th√®me et styles CSS personnalis√©s
st.markdown("""
<style>
    /* Variables de couleur */
    :root {
        --primary: #1E88E5;
        --secondary: #26A69A;
        --background: #FAFAFA;
        --accent: #FF7043;
        --text: #37474F;
        --light-grey: #ECEFF1;
        --dark-grey: #607D8B;
    }
    
    /* Styles globaux */
    .app-container {
        background-color: var(--background);
        color: var(--text);
        font-family: 'Inter', sans-serif;
    }
    
    /* En-t√™tes */
    .title {
        color: var(--primary);
        font-weight: 800;
        font-size: 2.5rem;
        margin-bottom: 0.5rem;
    }
    
    .subtitle {
        color: var(--dark-grey);
        font-weight: 600;
        font-size: 1.2rem;
        margin-bottom: 1.5rem;
    }
    
    .section-header {
        color: var(--primary);
        font-weight: 700;
        font-size: 1.8rem;
        margin: 1.5rem 0 1rem 0;
        padding-bottom: 0.5rem;
        border-bottom: 2px solid var(--light-grey);
    }
    
    .subsection-header {
        color: var(--secondary);
        font-weight: 600;
        font-size: 1.4rem;
        margin: 1rem 0 0.5rem 0;
    }
    
    /* Cartes et conteneurs */
    .card {
        background-color: white;
        border-radius: 10px;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        padding: 1.5rem;
        margin-bottom: 1.5rem;
    }
    
    .info-card {
        background-color: #E3F2FD;
        border-left: 5px solid var(--primary);
        padding: 1rem;
        border-radius: 5px;
        margin-bottom: 1.5rem;
    }
    
    .success-card {
        background-color: #E8F5E9;
        border-left: 5px solid #4CAF50;
        padding: 1rem;
        border-radius: 5px;
        margin-bottom: 1.5rem;
    }
    
    .warning-card {
        background-color: #FFF8E1;
        border-left: 5px solid #FFC107;
        padding: 1rem;
        border-radius: 5px;
        margin-bottom: 1.5rem;
    }
    
    /* Tableaux */
    .dataframe-container {
        border-radius: 5px;
        overflow: hidden;
        margin: 1rem 0;
    }
    
    /* Boutons */
    .stButton>button {
        background-color: var(--primary);
        color: white;
        border: none;
        border-radius: 5px;
        padding: 0.5rem 1rem;
        font-weight: 600;
        transition: all 0.3s;
    }
    
    .stButton>button:hover {
        background-color: #1976D2;
        box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
    }
    
    /* Indicateurs de progression */
    .metric-card {
        background-color: white;
        border-radius: 8px;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        padding: 1rem;
        text-align: center;
    }
    
    .metric-value {
        font-size: 2rem;
        font-weight: 700;
        color: var(--primary);
    }
    
    .metric-label {
        font-size: 0.9rem;
        color: var(--dark-grey);
    }
    
    /* Logo animation */
    @keyframes pulse {
        0% { transform: scale(1); }
        50% { transform: scale(1.05); }
        100% { transform: scale(1); }
    }
    
    .logo-animation {
        animation: pulse 2s infinite ease-in-out;
    }
    
    /* Hide Streamlit branding */
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    
    /* Custom scrollbar */
    ::-webkit-scrollbar {
        width: 10px;
    }
    
    ::-webkit-scrollbar-track {
        background: var(--light-grey);
    }
    
    ::-webkit-scrollbar-thumb {
        background: var(--dark-grey);
        border-radius: 5px;
    }
    
    ::-webkit-scrollbar-thumb:hover {
        background: #455A64;
    }
    
    /* Multiselect styling */
    .stMultiSelect > div[data-baseweb="select"] > div {
        background-color: white;
        border-radius: 5px;
    }
    
    /* Slider styling */
    .stSlider > div > div > div > div {
        color: var(--primary);
    }
</style>
""", unsafe_allow_html=True)

# Fonctions utilitaires
def generate_logo():
    """G√©n√®re un logo symbolique pour l'application."""
    fig = go.Figure()
    
    # Cercle principal
    fig.add_trace(go.Scatter(
        x=[0], y=[0],
        mode='markers',
        marker=dict(
            color='#1E88E5',
            size=25,
            line=dict(
                color='white',
                width=2
            )
        ),
        showlegend=False
    ))
    
    # Points repr√©sentant des √©l√©ments g√©ochimiques
    x = np.random.normal(0, 0.8, 20)
    y = np.random.normal(0, 0.8, 20)
    sizes = np.random.uniform(5, 15, 20)
    colors = np.random.uniform(0, 1, 20)
    
    fig.add_trace(go.Scatter(
        x=x, y=y,
        mode='markers',
        marker=dict(
            color=colors,
            colorscale='Viridis',
            size=sizes,
            line=dict(width=1, color='white')
        ),
        showlegend=False
    ))
    
    # Mise en page du logo
    fig.update_layout(
        width=100,
        height=100,
        margin=dict(l=0, r=0, t=0, b=0),
        paper_bgcolor='rgba(0,0,0,0)',
        plot_bgcolor='rgba(0,0,0,0)',
        xaxis=dict(
            showgrid=False,
            zeroline=False,
            visible=False,
            range=[-1.2, 1.2]
        ),
        yaxis=dict(
            showgrid=False,
            zeroline=False,
            visible=False,
            range=[-1.2, 1.2]
        )
    )
    
    return fig

@st.cache_data
def load_data(file):
    """Charge les donn√©es depuis un fichier CSV ou Excel."""
    try:
        if file.name.endswith('.csv'):
            df = pd.read_csv(file)
        elif file.name.endswith(('.xls', '.xlsx')):
            df = pd.read_excel(file)
        else:
            return None, "Format de fichier non support√©. Veuillez t√©l√©charger un fichier CSV ou Excel."
        
        # V√©rifications de base sur les donn√©es
        if df.empty:
            return None, "Le fichier est vide."
        
        # Statistiques sur les donn√©es
        stats = {
            "rows": len(df),
            "columns": len(df.columns),
            "numeric_columns": len(df.select_dtypes(include=['float64', 'int64']).columns),
            "missing_values": df.isnull().sum().sum(),
            "missing_percent": round((df.isnull().sum().sum() / (df.shape[0] * df.shape[1])) * 100, 2)
        }
        
        return df, stats
    except Exception as e:
        return None, f"Erreur lors du chargement des donn√©es: {e}"

def get_data_summary(df):
    """G√©n√®re un r√©sum√© statistique des donn√©es."""
    numeric_df = df.select_dtypes(include=['float64', 'int64'])
    
    summary = pd.DataFrame({
        'Min': numeric_df.min(),
        'Max': numeric_df.max(),
        'Moyenne': numeric_df.mean(),
        'M√©diane': numeric_df.median(),
        '√âcart-type': numeric_df.std(),
        'Valeurs manquantes': numeric_df.isnull().sum(),
        '% Valeurs manquantes': (numeric_df.isnull().sum() / len(df) * 100).round(2)
    })
    
    return summary

def plot_correlation_matrix(df, columns):
    """G√©n√®re une matrice de corr√©lation pour les colonnes s√©lectionn√©es."""
    corr_matrix = df[columns].corr()
    
    fig = px.imshow(
        corr_matrix,
        x=columns,
        y=columns,
        color_continuous_scale='RdBu_r',
        zmin=-1, zmax=1,
        title="Matrice de Corr√©lation"
    )
    
    fig.update_layout(
        height=600,
        width=700,
        xaxis_showgrid=False,
        yaxis_showgrid=False,
        coloraxis_colorbar=dict(
            title="Coefficient de Corr√©lation",
            thicknessmode="pixels", thickness=20,
            lenmode="pixels", len=400,
            yanchor="top", y=1,
            ticks="outside"
        )
    )
    
    return fig

def plot_histogram(df, column, bins=30):
    """G√©n√®re un histogramme pour une colonne sp√©cifique."""
    fig = px.histogram(
        df, x=column,
        nbins=bins,
        marginal="box",
        title=f"Distribution de {column}",
        color_discrete_sequence=['#1E88E5']
    )
    
    fig.update_layout(
        xaxis_title=column,
        yaxis_title="Fr√©quence",
        bargap=0.1
    )
    
    return fig

def plot_scatter_map(df, x_col, y_col, color_col=None, size_col=None, hover_cols=None):
    """G√©n√®re une carte des points d'√©chantillonnage."""
    if hover_cols is None:
        hover_cols = []
    
    fig = px.scatter(
        df, x=x_col, y=y_col,
        color=color_col,
        size=size_col,
        hover_data=hover_cols,
        title="Carte des √âchantillons",
        color_continuous_scale="Viridis"
    )
    
    fig.update_layout(
        xaxis_title=x_col,
        yaxis_title=y_col,
        height=600
    )
    
    return fig

def train_model(X, y, model_type, params=None):
    """Entra√Æne un mod√®le de r√©gression avec les param√®tres sp√©cifi√©s."""
    if params is None:
        params = {}
    
    if model_type == "XGBoost":
        model = xgb.XGBRegressor(objective='reg:squarederror', **params)
    elif model_type == "LightGBM":
        model = lgb.LGBMRegressor(**params)
    elif model_type == "RandomForest":
        model = RandomForestRegressor(**params)
    elif model_type == "GradientBoosting":
        model = GradientBoostingRegressor(**params)
    elif model_type == "KNN":
        model = KNeighborsRegressor(**params)
    else:
        raise ValueError(f"Type de mod√®le non support√©: {model_type}")
    
    model.fit(X, y)
    return model

def evaluate_model(model, X, y):
    """√âvalue un mod√®le de r√©gression et renvoie les m√©triques de performance."""
    y_pred = model.predict(X)
    
    mse = mean_squared_error(y, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y, y_pred)
    r2 = r2_score(y, y_pred)
    
    return {
        "MSE": mse,
        "RMSE": rmse,
        "MAE": mae,
        "R¬≤": r2,
        "Pr√©dictions": y_pred
    }

def cross_validate_model(model, X, y, cv=5):
    """Effectue une validation crois√©e et renvoie les m√©triques moyennes."""
    cv_scores = cross_val_score(model, X, y, cv=cv, scoring='neg_root_mean_squared_error')
    cv_rmse = -cv_scores
    
    cv_r2 = cross_val_score(model, X, y, cv=cv, scoring='r2')
    
    return {
        "CV RMSE": cv_rmse.mean(),
        "CV RMSE Std": cv_rmse.std(),
        "CV R¬≤": cv_r2.mean(),
        "CV R¬≤ Std": cv_r2.std()
    }

def detect_anomalies(X, contamination=0.1, method="IsolationForest", **kwargs):
    """D√©tecte les anomalies dans les donn√©es en utilisant diff√©rentes m√©thodes."""
    if method == "IsolationForest":
        model = IsolationForest(contamination=contamination, **kwargs)
        scores = model.fit_predict(X)
        # Conversion des scores (-1 pour anomalie, 1 pour normal) en binaire (1 pour anomalie, 0 pour normal)
        anomalies = np.where(scores == -1, 1, 0)
        anomaly_scores = -model.score_samples(X)
    
    elif method == "DBSCAN":
        model = DBSCAN(**kwargs)
        labels = model.fit_predict(X)
        # Les points avec label -1 sont des anomalies
        anomalies = np.where(labels == -1, 1, 0)
        
        # Calcul des scores d'anomalie bas√©s sur la distance au voisin le plus proche
        from sklearn.neighbors import NearestNeighbors
        nn = NearestNeighbors(n_neighbors=2)
        nn.fit(X)
        distances, _ = nn.kneighbors(X)
        anomaly_scores = distances[:, 1]  # Distance au voisin le plus proche
    
    elif method == "KMeans":
        model = KMeans(**kwargs)
        labels = model.fit_predict(X)
        distances = np.min(np.sqrt(np.sum((X - model.cluster_centers_[labels])**2, axis=1)), axis=0)
        
        # D√©finir un seuil pour les anomalies (par exemple, les 10% points les plus √©loign√©s)
        threshold = np.percentile(distances, 100 - contamination * 100)
        anomalies = np.where(distances > threshold, 1, 0)
        anomaly_scores = distances
    
    else:
        raise ValueError(f"M√©thode de d√©tection d'anomalies non support√©e: {method}")
    
    return anomalies, anomaly_scores

def generate_sampling_recommendations(df, target_cols, num_recommendations=10, min_distance=100, method="hybrid", exploration_weight=0.5):
    """G√©n√®re des recommandations pour de nouveaux points d'√©chantillonnage."""
    if 'X' not in df.columns or 'Y' not in df.columns:
        return None, "Les colonnes X et Y sont requises pour g√©n√©rer des recommandations."
    
    # D√©finition de la zone d'√©tude
    x_min, x_max = df['X'].min(), df['X'].max()
    y_min, y_max = df['Y'].min(), df['Y'].max()
    
    # Valeurs d'int√©r√™t
    target_values = df[target_cols].copy()
    
    # Normalisation
    target_values = (target_values - target_values.min()) / (target_values.max() - target_values.min())
    
    # Score combin√©
    df['interest_score'] = target_values.mean(axis=1)
    
    # Cr√©ation d'une grille de points potentiels
    grid_spacing = min_distance / 2
    x_grid = np.arange(x_min, x_max + grid_spacing, grid_spacing)
    y_grid = np.arange(y_min, y_max + grid_spacing, grid_spacing)
    xx, yy = np.meshgrid(x_grid, y_grid)
    grid_points = np.vstack([xx.ravel(), yy.ravel()]).T
    
    # Points existants
    existing_points = df[['X', 'Y']].values
    
    # Fonction pour calculer la distance minimale
    def min_distance_to_existing(point):
        distances = np.sqrt(np.sum((existing_points - point)**2, axis=1))
        return np.min(distances)
    
    # Calcul des scores
    grid_scores = []
    for point in grid_points:
        dist = min_distance_to_existing(point)
        
        # Ignorer les points trop proches
        if dist < min_distance / 2:
            grid_scores.append(-np.inf)
            continue
        
        # Score d'int√©r√™t
        if len(existing_points) > 0:
            weights = 1 / (np.sqrt(np.sum((existing_points - point)**2, axis=1)) + 1e-6)
            interest_interpolated = np.average(df['interest_score'], weights=weights)
        else:
            interest_interpolated = 0.5
        
        # Score d'exploration
        exploration_score = dist / min_distance
        
        # Score final
        if method == "value":
            score = interest_interpolated
        elif method == "exploration":
            score = exploration_score
        else:  # hybrid
            score = (1 - exploration_weight) * interest_interpolated + exploration_weight * exploration_score
        
        grid_scores.append(score)
    
    # Conversion en array
    grid_scores = np.array(grid_scores)
    
    # S√©lection des recommandations
    recommendations = []
    remaining_grid = grid_points.copy()
    remaining_scores = grid_scores.copy()
    
    for _ in range(num_recommendations):
        if len(remaining_grid) == 0 or np.all(np.isneginf(remaining_scores)):
            break
        
        # Meilleur point
        best_idx = np.argmax(remaining_scores)
        best_point = remaining_grid[best_idx]
        recommendations.append(best_point)
        
        # Mise √† jour des scores
        for i, point in enumerate(remaining_grid):
            dist = np.sqrt(np.sum((point - best_point)**2))
            if dist < min_distance:
                remaining_scores[i] = -np.inf
    
    # Cr√©ation du DataFrame
    rec_df = pd.DataFrame(recommendations, columns=['X', 'Y'])
    rec_df['Rang'] = range(1, len(rec_df) + 1)
    
    # Estimation des valeurs pour les √©l√©ments d'int√©r√™t
    for element in target_cols:
        rec_df[f"{element}_estim√©"] = np.nan
        
        for i, point in rec_df.iterrows():
            distances = np.sqrt(np.sum((existing_points - [point['X'], point['Y']])**2, axis=1))
            weights = 1 / (distances + 1e-6)
            rec_df.loc[i, f"{element}_estim√©"] = np.average(df[element], weights=weights)
    
    # Grille pour la heatmap
    grid_scores_2d = np.full((len(y_grid), len(x_grid)), np.nan)
    valid_indices = ~np.isinf(grid_scores)
    grid_scores_2d.flat[valid_indices] = grid_scores[valid_indices]
    
    return rec_df, {
        "grid_x": x_grid,
        "grid_y": y_grid,
        "grid_scores": grid_scores_2d
    }

def interpolate_values(df, value_col, grid_size=100, method='linear'):
    """Cr√©e une grille d'interpolation pour visualiser la distribution spatiale des valeurs."""
    if 'X' not in df.columns or 'Y' not in df.columns:
        return None
    
    # D√©finition de la grille
    x_min, x_max = df['X'].min(), df['X'].max()
    y_min, y_max = df['Y'].min(), df['Y'].max()
    
    # Extension de la grille de 5% pour une meilleure visualisation
    x_range = x_max - x_min
    y_range = y_max - y_min
    
    x_min -= 0.05 * x_range
    x_max += 0.05 * x_range
    y_min -= 0.05 * y_range
    y_max += 0.05 * y_range
    
    # Cr√©ation de la grille
    xi = np.linspace(x_min, x_max, grid_size)
    yi = np.linspace(y_min, y_max, grid_size)
    xi_grid, yi_grid = np.meshgrid(xi, yi)
    
    # Points et valeurs pour l'interpolation
    points = df[['X', 'Y']].values
    values = df[value_col].values
    
    # Interpolation
    zi = griddata(points, values, (xi_grid, yi_grid), method=method)
    
    return {
        "x": xi,
        "y": yi,
        "z": zi
    }

def plot_contour_map(interp_data, df, value_col, colorscale='Viridis'):
    """G√©n√®re une carte de contour interpol√©e avec les points d'√©chantillonnage."""
    # Cr√©ation de la figure de base
    fig = go.Figure()
    
    # Ajout du contour
    fig.add_trace(go.Contour(
        z=interp_data["z"],
        x=interp_data["x"],
        y=interp_data["y"],
        colorscale=colorscale,
        colorbar=dict(title=value_col),
        line=dict(width=0.5),
        contours=dict(
            showlabels=True,
            labelfont=dict(size=10, color='white')
        )
    ))
    
    # Ajout des points d'√©chantillonnage
    fig.add_trace(go.Scatter(
        x=df['X'],
        y=df['Y'],
        mode='markers',
        marker=dict(
            color=df[value_col],
            colorscale=colorscale,
            size=8,
            line=dict(color='black', width=1),
            showscale=False
        ),
        name='Points d\'√©chantillonnage'
    ))
    
    # Mise en page
    fig.update_layout(
        title=f"Carte de Contour de {value_col}",
        xaxis_title='X',
        yaxis_title='Y',
        height=600,
        width=800
    )
    
    return fig

def plot_3d_surface(interp_data, value_col, colorscale='Viridis'):
    """G√©n√®re une surface 3D pour visualiser la distribution spatiale des valeurs."""
    fig = go.Figure(data=[go.Surface(
        z=interp_data["z"],
        x=interp_data["x"],
        y=interp_data["y"],
        colorscale=colorscale,
        colorbar=dict(title=value_col)
    )])
    
    fig.update_layout(
        title=f"Surface 3D de {value_col}",
        scene=dict(
            xaxis_title='X',
            yaxis_title='Y',
            zaxis_title=value_col,
            aspectratio=dict(x=1, y=1, z=0.5)
        ),
        height=700,
        width=800
    )
    
    return fig

def create_voronoi_diagram(df, value_col, colorscale='Viridis'):
    """Cr√©e un diagramme de Voronoi color√© selon les valeurs d'une colonne."""
    # Points pour le diagramme de Voronoi
    points = df[['X', 'Y']].values
    
    # Calcul du diagramme de Voronoi
    vor = Voronoi(points)
    
    # Cr√©ation de la figure
    fig, ax = plt.subplots(figsize=(10, 8))
    
    # Tracer le diagramme de Voronoi
    voronoi_plot_2d(vor, ax=ax, show_vertices=False, line_colors='gray', line_width=1, line_alpha=0.5, point_size=0)
    
    # Colorier les points selon leurs valeurs
    scatter = ax.scatter(points[:,0], points[:,1], c=df[value_col], cmap=colorscale, s=50, alpha=0.8, edgecolors='black')
    
    # Ajouter une barre de couleur
    plt.colorbar(scatter, label=value_col)
    
    # Ajustements
    ax.set_title(f"Diagramme de Voronoi - {value_col}")
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.grid(True, linestyle='--', alpha=0.7)
    
    # Convertir la figure en image pour Streamlit
    buf = BytesIO()
    plt.tight_layout()
    plt.savefig(buf, format="png")
    buf.seek(0)
    img_str = base64.b64encode(buf.read()).decode()
    plt.close(fig)
    
    return f'<img src="data:image/png;base64,{img_str}" alt="Diagramme de Voronoi">'

# Personnaliser le sidebar
def customize_sidebar():
    logo_fig = generate_logo()
    
    with st.sidebar:
        # Logo et titre
        col1, col2 = st.columns([1, 3])
        with col1:
            st.plotly_chart(logo_fig, use_container_width=True)
        with col2:
            st.markdown("<h1 style='color:#1E88E5; margin-bottom:0; font-size:1.8em;'>GeoChem</h1>", unsafe_allow_html=True)
            st.markdown("<h2 style='color:#26A69A; margin-top:0; font-size:1.4em;'>Predictor</h2>", unsafe_allow_html=True)
        
        st.markdown("""---""")
        
        # Navigation
        st.subheader("üß≠ Navigation")
        page = st.radio(
            label="",
            options=["üìä Tableau de Bord", "üîç Analyse Exploratoire", "üîÆ Pr√©diction de Min√©ralisation", 
                     "üõë D√©tection d'Anomalies", "üéØ Recommandation de Cibles"],
            label_visibility="collapsed"
        )
        
        st.markdown("""---""")
        
        # Informations
        with st.expander("‚ÑπÔ∏è √Ä propos"):
            st.markdown("""
            **GeoChem Predictor** est un outil d'analyse g√©ochimique automatis√© d√©velopp√© par Didier Ouedraogo, P.Geo.
            
            Cette application permet d'analyser des donn√©es g√©ochimiques, de pr√©dire la min√©ralisation, de d√©tecter des anomalies et de recommander des cibles pour de nouveaux pr√©l√®vements.
            
            Version: 2.0.1
            """)
        
        # Cr√©dit
        st.markdown("""
        <div style='position:fixed; bottom:10px; left:20px; font-size:0.8em; color:#607D8B;'>
            D√©velopp√© par<br/>Didier Ouedraogo, P.Geo
        </div>
        """, unsafe_allow_html=True)
    
    return page

# Fonction pour enregistrer l'√©tat de session
def initialize_session_state():
    if 'data' not in st.session_state:
        st.session_state.data = None
    if 'data_stats' not in st.session_state:
        st.session_state.data_stats = None
    if 'uploaded_file_name' not in st.session_state:
        st.session_state.uploaded_file_name = None
    if 'model' not in st.session_state:
        st.session_state.model = None
    if 'model_features' not in st.session_state:
        st.session_state.model_features = None
    if 'model_target' not in st.session_state:
        st.session_state.model_target = None
    if 'model_metrics' not in st.session_state:
        st.session_state.model_metrics = None
    if 'session_id' not in st.session_state:
        st.session_state.session_id = str(uuid.uuid4())

# Composant pour t√©l√©charger des donn√©es
def upload_data_component():
    st.markdown("<h2 class='section-header'>üì§ Charger des Donn√©es</h2>", unsafe_allow_html=True)
    
    col1, col2 = st.columns([3, 1])
    
    with col1:
        uploaded_file = st.file_uploader(
            "T√©l√©charger un fichier CSV ou Excel contenant vos donn√©es g√©ochimiques",
            type=["csv", "xlsx", "xls"],
            help="Vos donn√©es doivent contenir des coordonn√©es (X, Y) et des valeurs g√©ochimiques pour diff√©rents √©l√©ments"
        )
    
    with col2:
        st.markdown("<div style='height: 20px;'></div>", unsafe_allow_html=True)
        use_example = st.checkbox("Utiliser des donn√©es d'exemple", value=False)
    
    if use_example:
        # G√©n√©rer des donn√©es d'exemple
        np.random.seed(42)
        n_samples = 100
        x = np.random.uniform(low=350000, high=355000, size=n_samples)
        y = np.random.uniform(low=7650000, high=7655000, size=n_samples)
        
        # Simuler une zone min√©ralis√©e au centre
        center_x, center_y = 352500, 7652500
        distance = np.sqrt((x - center_x)**2 + (y - center_y)**2)
        max_dist = np.max(distance)
        
        # Valeurs d'or qui diminuent avec la distance du centre
        au_base = 10 * np.exp(-3 * distance / max_dist)
        au = au_base + np.random.lognormal(mean=0, sigma=0.5, size=n_samples)
        
        # Valeurs d'arsenic corr√©l√©es avec l'or
        as_base = 50 * np.exp(-2 * distance / max_dist)
        as_values = as_base + np.random.lognormal(mean=0, sigma=0.7, size=n_samples)
        
        # Valeurs de cuivre avec une autre zone d'int√©r√™t
        center2_x, center2_y = 354000, 7653500
        distance2 = np.sqrt((x - center2_x)**2 + (y - center2_y)**2)
        cu_base = 200 * np.exp(-4 * distance2 / max_dist)
        cu = cu_base + np.random.lognormal(mean=0, sigma=0.6, size=n_samples)
        
        # Valeurs d'antimoine faiblement corr√©l√©es avec l'or
        sb_base = 2 * np.exp(-2.5 * distance / max_dist)
        sb = sb_base + np.random.lognormal(mean=0, sigma=0.8, size=n_samples)
        
        # Zinc al√©atoire (non corr√©l√©)
        zn = np.random.lognormal(mean=3.5, sigma=0.4, size=n_samples)
        
        # Cr√©er le DataFrame
        example_data = pd.DataFrame({
            'X': x,
            'Y': y,
            'Au_ppb': au,
            'As_ppm': as_values,
            'Cu_ppm': cu,
            'Sb_ppm': sb,
            'Zn_ppm': zn
        })
        
        st.session_state.data = example_data
        st.session_state.data_stats = {
            "rows": len(example_data),
            "columns": len(example_data.columns),
            "numeric_columns": len(example_data.select_dtypes(include=['float64', 'int64']).columns),
            "missing_values": 0,
            "missing_percent": 0
        }
        st.session_state.uploaded_file_name = "donnees_exemple.csv"
        
        st.success("‚úÖ Donn√©es d'exemple charg√©es avec succ√®s!")
        
    elif uploaded_file is not None:
        data, result = load_data(uploaded_file)
        
        if isinstance(result, dict):
            # Chargement r√©ussi
            st.session_state.data = data
            st.session_state.data_stats = result
            st.session_state.uploaded_file_name = uploaded_file.name
            
            st.success(f"‚úÖ Fichier '{uploaded_file.name}' charg√© avec succ√®s!")
        else:
            # Erreur lors du chargement
            st.error(result)
    
    # Afficher un aper√ßu des donn√©es si disponibles
    if st.session_state.data is not None:
        with st.expander("üìã Aper√ßu des Donn√©es", expanded=True):
            st.dataframe(st.session_state.data.head(10), use_container_width=True)
            
            # Afficher les statistiques du jeu de donn√©es
            stats = st.session_state.data_stats
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("√âchantillons", stats["rows"])
            
            with col2:
                st.metric("Variables", stats["columns"])
            
            with col3:
                st.metric("Variables num√©riques", stats["numeric_columns"])
            
            with col4:
                st.metric("Valeurs manquantes", f"{stats['missing_percent']}%")
                
        # Option pour r√©initialiser les donn√©es
        if st.button("üóëÔ∏è R√©initialiser les donn√©es"):
            st.session_state.data = None
            st.session_state.data_stats = None
            st.session_state.uploaded_file_name = None
            st.session_state.model = None
            st.session_state.model_features = None
            st.session_state.model_target = None
            st.session_state.model_metrics = None
            st.rerun()

# Interface principale
def main():
    # Initialiser l'√©tat de la session
    initialize_session_state()
    
    # Personnaliser la barre lat√©rale et obtenir la page s√©lectionn√©e
    page = customize_sidebar()
    
    # Interface principale
    if page == "üìä Tableau de Bord":
        show_dashboard()
    elif page == "üîç Analyse Exploratoire":
        show_exploratory_analysis()
    elif page == "üîÆ Pr√©diction de Min√©ralisation":
        show_prediction()
    elif page == "üõë D√©tection d'Anomalies":
        show_anomaly_detection()
    elif page == "üéØ Recommandation de Cibles":
        show_recommendation()

# Pages de l'application
def show_dashboard():
    st.markdown("<h1 class='title'>Tableau de Bord</h1>", unsafe_allow_html=True)
    st.markdown("<p class='subtitle'>Vue d'ensemble des donn√©es g√©ochimiques et outils d'analyse</p>", unsafe_allow_html=True)
    
    # Composant pour t√©l√©charger des donn√©es
    upload_data_component()
    
    # Si des donn√©es sont charg√©es, afficher les analyses de base
    if st.session_state.data is not None:
        df = st.session_state.data
        
        # R√©sum√© statistique
        st.markdown("<h2 class='section-header'>üìä R√©sum√© Statistique</h2>", unsafe_allow_html=True)
        
        # Obtenir le r√©sum√© des donn√©es
        summary = get_data_summary(df)
        
        # Afficher le r√©sum√© dans un tableau interactif
        st.dataframe(summary, use_container_width=True)
        
        # Visualisations rapides
        st.markdown("<h2 class='section-header'>üîç Visualisations Rapides</h2>", unsafe_allow_html=True)
        
        # Obtenir les colonnes num√©riques
        numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()
        
        # S√©lection de visualisation
        viz_type = st.selectbox(
            "S√©lectionner le type de visualisation",
            ["Carte des √©chantillons", "Matrice de corr√©lation", "Histogrammes", "Bo√Ætes √† moustaches"]
        )
        
        if viz_type == "Carte des √©chantillons":
            if 'X' in df.columns and 'Y' in df.columns:
                st.markdown("<div class='card'>", unsafe_allow_html=True)
                
                col1, col2 = st.columns([3, 1])
                
                with col2:
                    color_col = st.selectbox("Colorer par", [None] + numeric_cols)
                    size_col = st.selectbox("Dimensionner par", [None] + numeric_cols)
                    
                with col1:
                    fig = plot_scatter_map(df, 'X', 'Y', color_col, size_col)
                    st.plotly_chart(fig, use_container_width=True)
                
                st.markdown("</div>", unsafe_allow_html=True)
            else:
                st.warning("Les colonnes X et Y sont n√©cessaires pour afficher la carte des √©chantillons.")
        
        elif viz_type == "Matrice de corr√©lation":
            st.markdown("<div class='card'>", unsafe_allow_html=True)
            
            # S√©lection des variables pour la matrice de corr√©lation
            selected_cols = st.multiselect(
                "S√©lectionner les variables pour la matrice de corr√©lation",
                numeric_cols,
                default=numeric_cols[:min(5, len(numeric_cols))]
            )
            
            if len(selected_cols) > 1:
                fig = plot_correlation_matrix(df, selected_cols)
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.info("S√©lectionnez au moins deux variables pour afficher la matrice de corr√©lation.")
            
            st.markdown("</div>", unsafe_allow_html=True)
        
        elif viz_type == "Histogrammes":
            st.markdown("<div class='card'>", unsafe_allow_html=True)
            
            col1, col2 = st.columns([3, 1])
            
            with col2:
                hist_col = st.selectbox("Variable √† visualiser", numeric_cols)
                bins = st.slider("Nombre de bins", 5, 100, 30)
                
            with col1:
                fig = plot_histogram(df, hist_col, bins=bins)
                st.plotly_chart(fig, use_container_width=True)
            
            st.markdown("</div>", unsafe_allow_html=True)
        
        elif viz_type == "Bo√Ætes √† moustaches":
            st.markdown("<div class='card'>", unsafe_allow_html=True)
            
            selected_cols = st.multiselect(
                "S√©lectionner les variables pour les bo√Ætes √† moustaches",
                numeric_cols,
                default=numeric_cols[:min(5, len(numeric_cols))]
            )
            
            if selected_cols:
                fig = px.box(df, y=selected_cols, title="Distribution des Variables")
                st.plotly_chart(fig, use_container_width=True)
            
            st.markdown("</div>", unsafe_allow_html=True)
        
        # R√©sum√© des √©l√©ments-cl√©s
        if 'X' in df.columns and 'Y' in df.columns and len(numeric_cols) > 2:
            st.markdown("<h2 class='section-header'>üí° Aper√ßu Rapide</h2>", unsafe_allow_html=True)
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("<div class='card'>", unsafe_allow_html=True)
                st.subheader("üîù Valeurs maximales")
                
                # Trouver l'emplacement des valeurs maximales pour chaque variable
                max_values = pd.DataFrame(columns=['Variable', 'Valeur Max', 'X', 'Y'])
                
                for col in numeric_cols[:5]:  # Limiter √† 5 variables pour la lisibilit√©
                    if col not in ['X', 'Y']:
                        max_idx = df[col].idxmax()
                        max_values = pd.concat([max_values, pd.DataFrame({
                            'Variable': [col],
                            'Valeur Max': [df.loc[max_idx, col]],
                            'X': [df.loc[max_idx, 'X']],
                            'Y': [df.loc[max_idx, 'Y']]
                        })])
                
                st.dataframe(max_values, use_container_width=True)
                st.markdown("</div>", unsafe_allow_html=True)
            
            with col2:
                st.markdown("<div class='card'>", unsafe_allow_html=True)
                st.subheader("üîÑ Corr√©lations principales")
                
                # Calculer les corr√©lations
                corr_matrix = df[numeric_cols].corr()
                
                # Obtenir les paires de variables avec les corr√©lations les plus fortes (en valeur absolue)
                corr_pairs = []
                
                for i in range(len(numeric_cols)):
                    for j in range(i+1, len(numeric_cols)):
                        col1_name = numeric_cols[i]
                        col2_name = numeric_cols[j]
                        
                        if col1_name != 'X' and col1_name != 'Y' and col2_name != 'X' and col2_name != 'Y':
                            corr_value = corr_matrix.loc[col1_name, col2_name]
                            corr_pairs.append((col1_name, col2_name, corr_value))
                
                # Trier par valeur absolue de corr√©lation
                corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)
                
                # Afficher les 5 premi√®res paires
                corr_df = pd.DataFrame(
                    [(f"{pair[0]} - {pair[1]}", f"{pair[2]:.3f}") for pair in corr_pairs[:5]],
                    columns=["Paire de variables", "Corr√©lation"]
                )
                
                st.dataframe(corr_df, use_container_width=True)
                st.markdown("</div>", unsafe_allow_html=True)
    else:
        # Si aucune donn√©e n'est charg√©e, afficher un message et des exemples
        st.markdown("<div class='info-card'>", unsafe_allow_html=True)
        st.markdown("""
        ### üëã Bienvenue dans GeoChem Predictor!
        
        Pour commencer, t√©l√©chargez vos donn√©es g√©ochimiques ou utilisez les donn√©es d'exemple.
        
        Cette application vous permet de:
        - Analyser vos donn√©es g√©ochimiques
        - Pr√©dire la min√©ralisation
        - D√©tecter des anomalies
        - G√©n√©rer des recommandations pour de futurs √©chantillonnages
        """)
        st.markdown("</div>", unsafe_allow_html=True)
        
        # Afficher quelques exemples de ce que l'application peut faire
        st.markdown("<h2 class='section-header'>‚ú® Fonctionnalit√©s</h2>", unsafe_allow_html=True)
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.markdown("<div class='card'>", unsafe_allow_html=True)
            st.markdown("### üîÆ Pr√©diction")
            st.image("https://raw.githubusercontent.com/streamlit/streamlit/master/examples/data/bike_rentals_visualization.jpg")
            st.markdown("""
            Pr√©disez la min√©ralisation √† partir de donn√©es g√©ochimiques gr√¢ce √† des mod√®les de r√©gression avanc√©s.
            """)
            st.markdown("</div>", unsafe_allow_html=True)
        
        with col2:
            st.markdown("<div class='card'>", unsafe_allow_html=True)
            st.markdown("### üõë Anomalies")
            st.image("https://raw.githubusercontent.com/streamlit/streamlit/master/examples/data/chart_data_anomalies.jpg")
            st.markdown("""
            Identifiez automatiquement les anomalies g√©ochimiques √† l'aide de l'apprentissage non supervis√©.
            """)
            st.markdown("</div>", unsafe_allow_html=True)
        
        with col3:
            st.markdown("<div class='card'>", unsafe_allow_html=True)
            st.markdown("### üéØ Recommandations")
            st.image("https://raw.githubusercontent.com/streamlit/streamlit/master/examples/data/map_data.jpg")
            st.markdown("""
            Obtenez des suggestions pour les emplacements optimaux des prochains pr√©l√®vements.
            """)
            st.markdown("</div>", unsafe_allow_html=True)

def show_exploratory_analysis():
    st.markdown("<h1 class='title'>Analyse Exploratoire</h1>", unsafe_allow_html=True)
    st.markdown("<p class='subtitle'>Explorez et visualisez vos donn√©es g√©ochimiques en profondeur</p>", unsafe_allow_html=True)
    
    # Composant pour t√©l√©charger des donn√©es si n√©cessaire
    if st.session_state.data is None:
        upload_data_component()
    else:
        # Bouton pour changer de jeu de donn√©es
        if st.button("üì§ Changer de jeu de donn√©es"):
            upload_data_component()
    
    # Si des donn√©es sont charg√©es, afficher les outils d'analyse exploratoire
    if st.session_state.data is not None:
        df = st.session_state.data
        
        # Informations sur le jeu de donn√©es
        st.markdown(f"<div class='info-card'>Jeu de donn√©es actuel: <strong>{st.session_state.uploaded_file_name}</strong> | {len(df)} √©chantillons | {len(df.columns)} variables</div>", unsafe_allow_html=True)
        
        # Navigation par onglets pour diff√©rentes visualisations
        tab1, tab2, tab3, tab4 = st.tabs(["üìä Distribution", "üó∫Ô∏è Cartes", "üîÑ Relations", "üìà Tendances"])
        
        # Obtenir les colonnes num√©riques
        numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()
        
        with tab1:
            st.markdown("<h2 class='subsection-header'>Distribution des Variables</h2>", unsafe_allow_html=True)
            
            # S√©lection de variable et type de graphique
            col1, col2 = st.columns([1, 3])
            
            with col1:
                dist_var = st.selectbox("S√©lectionnez une variable", numeric_cols)
                dist_type = st.radio("Type de visualisation", ["Histogramme", "Bo√Æte √† moustaches", "Violin Plot", "ECDF"])
                
                # Options avanc√©es
                with st.expander("Options avanc√©es"):
                    if dist_type == "Histogramme":
                        bins = st.slider("Nombre de bins", 5, 100, 30)
                        use_log = st.checkbox("√âchelle logarithmique", value=False)
                    
                    norm_test = st.checkbox("Test de normalit√©", value=False)
            
            with col2:
                if dist_type == "Histogramme":
                    fig = px.histogram(
                        df, x=dist_var,
                        nbins=bins,
                        marginal="box",
                        title=f"Distribution de {dist_var}",
                        log_x=use_log,
                        color_discrete_sequence=['#1E88E5']
                    )
                    st.plotly_chart(fig, use_container_width=True)
                
                elif dist_type == "Bo√Æte √† moustaches":
                    fig = px.box(
                        df, y=dist_var,
                        title=f"Bo√Æte √† moustaches de {dist_var}",
                        points="all",
                        color_discrete_sequence=['#1E88E5']
                    )
                    st.plotly_chart(fig, use_container_width=True)
                
                elif dist_type == "Violin Plot":
                    fig = px.violin(
                        df, y=dist_var,
                        title=f"Violin Plot de {dist_var}",
                        box=True,
                        points="all",
                        color_discrete_sequence=['#1E88E5']
                    )
                    st.plotly_chart(fig, use_container_width=True)
                
                elif dist_type == "ECDF":
                    # Fonction de distribution cumulative empirique
                    sorted_data = np.sort(df[dist_var].dropna())
                    y = np.arange(1, len(sorted_data) + 1) / len(sorted_data)
                    
                    fig = px.line(
                        x=sorted_data, y=y,
                        title=f"Distribution cumulative de {dist_var}",
                        labels={"x": dist_var, "y": "Probabilit√© cumul√©e"}
                    )
                    
                    fig.update_traces(mode='lines', line=dict(color='#1E88E5', width=2))
                    st.plotly_chart(fig, use_container_width=True)
            
            # Tests statistiques
            if norm_test:
                from scipy import stats
                
                # Test de normalit√© (Shapiro-Wilk pour petits √©chantillons, D'Agostino-Pearson pour grands √©chantillons)
                data_no_na = df[dist_var].dropna()
                
                st.markdown("<div class='card'>", unsafe_allow_html=True)
                st.markdown("#### Tests de Normalit√©")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    if len(data_no_na) <= 5000:  # Shapiro-Wilk est limit√© √† ~5000 √©chantillons
                        shapiro_stat, shapiro_p = stats.shapiro(data_no_na)
                        st.metric("Test de Shapiro-Wilk (p-value)", f"{shapiro_p:.6f}")
                        
                        if shapiro_p < 0.05:
                            st.markdown("‚ùå Les donn√©es ne suivent pas une distribution normale.")
                        else:
                            st.markdown("‚úÖ Les donn√©es suivent une distribution normale.")
                
                with col2:
                    # Test D'Agostino-Pearson
                    k2, p_value = stats.normaltest(data_no_na)
                    st.metric("Test D'Agostino-Pearson (p-value)", f"{p_value:.6f}")
                    
                    if p_value < 0.05:
                        st.markdown("‚ùå Les donn√©es ne suivent pas une distribution normale.")
                    else:
                        st.markdown("‚úÖ Les donn√©es suivent une distribution normale.")
                
                # Statistiques descriptives
                st.markdown("#### Statistiques descriptives")
                
                stats_df = pd.DataFrame({
                    'Statistique': ['Moyenne', 'M√©diane', '√âcart-type', 'Min', 'Max', 'Q1 (25%)', 'Q3 (75%)', 'Skewness', 'Kurtosis'],
                    'Valeur': [
                        f"{data_no_na.mean():.4f}",
                        f"{data_no_na.median():.4f}",
                        f"{data_no_na.std():.4f}",
                        f"{data_no_na.min():.4f}",
                        f"{data_no_na.max():.4f}",
                        f"{data_no_na.quantile(0.25):.4f}",
                        f"{data_no_na.quantile(0.75):.4f}",
                        f"{stats.skew(data_no_na):.4f}",
                        f"{stats.kurtosis(data_no_na):.4f}"
                    ]
                })
                
                st.dataframe(stats_df, use_container_width=True)
                st.markdown("</div>", unsafe_allow_html=True)
        
        with tab2:
            if 'X' in df.columns and 'Y' in df.columns:
                st.markdown("<h2 class='subsection-header'>Cartes et Visualisations Spatiales</h2>", unsafe_allow_html=True)
                
                # S√©lection du type de carte
                map_type = st.selectbox(
                    "Type de carte",
                    ["Carte des points", "Carte de chaleur", "Carte de contour", "Surface 3D", "Diagramme de Voronoi"]
                )
                
                col1, col2 = st.columns([3, 1])
                
                with col2:
                    # Options communes √† toutes les cartes
                    map_var = st.selectbox("Variable √† visualiser", [col for col in numeric_cols if col not in ['X', 'Y']])
                    colorscale = st.selectbox("Palette de couleurs", ["Viridis", "Plasma", "Inferno", "Magma", "Cividis", "Turbo", "RdBu", "Spectral"])
                    
                    # Options sp√©cifiques au type de carte
                    if map_type in ["Carte de contour", "Surface 3D"]:
                        interp_method = st.selectbox("M√©thode d'interpolation", ["linear", "cubic", "nearest"])
                    
                    if map_type == "Carte de chaleur":
                        resolution = st.slider("R√©solution", 50, 200, 100)
                
                with col1:
                    if map_type == "Carte des points":
                        fig = px.scatter(
                            df, x='X', y='Y',
                            color=map_var,
                            size=map_var,
                            hover_data=[map_var],
                            color_continuous_scale=colorscale,
                            title=f"Carte des √©chantillons - {map_var}"
                        )
                        
                        st.plotly_chart(fig, use_container_width=True)
                    
                    elif map_type == "Carte de chaleur":
                        # Cr√©er une grille r√©guli√®re
                        x_range = np.linspace(df['X'].min(), df['X'].max(), resolution)
                        y_range = np.linspace(df['Y'].min(), df['Y'].max(), resolution)
                        xx, yy = np.meshgrid(x_range, y_range)
                        
                        # Interpolation
                        from scipy.interpolate import griddata
                        z = griddata((df['X'], df['Y']), df[map_var], (xx, yy), method='linear')
                        
                        # Cr√©er la carte de chaleur
                        fig = go.Figure(data=go.Heatmap(
                            z=z,
                            x=x_range,
                            y=y_range,
                            colorscale=colorscale,
                            colorbar=dict(title=map_var)
                        ))
                        
                        # Ajouter les points d'√©chantillonnage
                        fig.add_trace(go.Scatter(
                            x=df['X'],
                            y=df['Y'],
                            mode='markers',
                            marker=dict(color='black', size=4),
                            showlegend=False
                        ))
                        
                        fig.update_layout(
                            title=f"Carte de chaleur - {map_var}",
                            xaxis_title='X',
                            yaxis_title='Y'
                        )
                        
                        st.plotly_chart(fig, use_container_width=True)
                    
                    elif map_type == "Carte de contour":
                        # Interpolation
                        interp_data = interpolate_values(df, map_var, grid_size=100, method=interp_method)
                        
                        if interp_data:
                            fig = plot_contour_map(interp_data, df, map_var, colorscale=colorscale)
                            st.plotly_chart(fig, use_container_width=True)
                    
                    elif map_type == "Surface 3D":
                        # Interpolation
                        interp_data = interpolate_values(df, map_var, grid_size=100, method=interp_method)
                        
                        if interp_data:
                            fig = plot_3d_surface(interp_data, map_var, colorscale=colorscale)
                            st.plotly_chart(fig, use_container_width=True)
                    
                    elif map_type == "Diagramme de Voronoi":
                        # G√©n√©rer le diagramme de Voronoi
                        voronoi_html = create_voronoi_diagram(df, map_var, colorscale=colorscale.lower())
                        st.markdown(voronoi_html, unsafe_allow_html=True)
            else:
                st.warning("Les colonnes X et Y sont n√©cessaires pour les visualisations spatiales.")
        
        with tab3:
            st.markdown("<h2 class='subsection-header'>Relations entre Variables</h2>", unsafe_allow_html=True)
            
            # Type de visualisation
            relation_type = st.selectbox(
                "Type de visualisation",
                ["Nuage de points", "Matrice de corr√©lation", "Pairplot", "Heatmap"]
            )
            
            if relation_type == "Nuage de points":
                col1, col2 = st.columns([3, 1])
                
                with col2:
                    x_var = st.selectbox("Variable X", numeric_cols, index=0)
                    y_var = st.selectbox("Variable Y", numeric_cols, index=min(1, len(numeric_cols)-1))
                    
                    color_var = st.selectbox("Colorer par", [None] + numeric_cols)
                    trendline = st.checkbox("Ajouter une ligne de tendance", value=True)
                
                with col1:
                    if trendline:
                        fig = px.scatter(
                            df, x=x_var, y=y_var,
                            color=color_var,
                            trendline="ols",
                            trendline_color_override="red",
                            title=f"Relation entre {x_var} et {y_var}"
                        )
                    else:
                        fig = px.scatter(
                            df, x=x_var, y=y_var,
                            color=color_var,
                            title=f"Relation entre {x_var} et {y_var}"
                        )
                    
                    st.plotly_chart(fig, use_container_width=True)
                
                # Statistiques de corr√©lation
                if trendline:
                    st.markdown("<div class='card'>", unsafe_allow_html=True)
                    st.markdown("#### Analyse de corr√©lation")
                    
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        pearson_r = df[x_var].corr(df[y_var], method='pearson')
                        st.metric("Coefficient de Pearson", f"{pearson_r:.4f}")
                    
                    with col2:
                        spearman_r = df[x_var].corr(df[y_var], method='spearman')
                        st.metric("Rho de Spearman", f"{spearman_r:.4f}")
                    
                    with col3:
                        kendall_tau = df[x_var].corr(df[y_var], method='kendall')
                        st.metric("Tau de Kendall", f"{kendall_tau:.4f}")
                    
                    # Interpr√©tation automatique
                    st.markdown("#### Interpr√©tation")
                    
                    abs_r = abs(pearson_r)
                    if abs_r < 0.3:
                        st.markdown("üìä **Corr√©lation faible** entre les variables.")
                    elif abs_r < 0.7:
                        st.markdown("üìä **Corr√©lation mod√©r√©e** entre les variables.")
                    else:
                        st.markdown("üìä **Corr√©lation forte** entre les variables.")
                    
                    if pearson_r > 0:
                        st.markdown("üìà La relation est **positive** : quand une variable augmente, l'autre tend √† augmenter aussi.")
                    else:
                        st.markdown("üìâ La relation est **n√©gative** : quand une variable augmente, l'autre tend √† diminuer.")
                    
                    # Diff√©rence Pearson vs Spearman
                    diff = abs(pearson_r) - abs(spearman_r)
                    if abs(diff) > 0.1:
                        st.markdown("‚ö†Ô∏è **√âcart notable** entre les corr√©lations de Pearson et de Spearman, sugg√©rant une relation non lin√©aire ou la pr√©sence de valeurs aberrantes.")
                    
                    st.markdown("</div>", unsafe_allow_html=True)
            
            elif relation_type == "Matrice de corr√©lation":
                # S√©lection des variables
                selected_vars = st.multiselect(
                    "S√©lectionner les variables pour la matrice de corr√©lation",
                    numeric_cols,
                    default=numeric_cols[:min(6, len(numeric_cols))]
                )
                
                if len(selected_vars) > 1:
                    corr_method = st.radio("M√©thode de corr√©lation", ["pearson", "spearman", "kendall"], horizontal=True)
                    
                    # Calculer la matrice de corr√©lation
                    corr_matrix = df[selected_vars].corr(method=corr_method)
                    
                    # Cr√©er la heatmap
                    fig = px.imshow(
                        corr_matrix,
                        text_auto=True,
                        color_continuous_scale="RdBu_r",
                        zmin=-1, zmax=1,
                        title=f"Matrice de corr√©lation ({corr_method.capitalize()})"
                    )
                    
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # Analyse des corr√©lations principales
                    st.markdown("<div class='card'>", unsafe_allow_html=True)
                    st.markdown("#### Corr√©lations principales")
                    
                    # Mettre en forme les r√©sultats pour une meilleure lisibilit√©
                    corr_pairs = []
                    
                    for i in range(len(selected_vars)):
                        for j in range(i+1, len(selected_vars)):
                            var1 = selected_vars[i]
                            var2 = selected_vars[j]
                            corr_value = corr_matrix.loc[var1, var2]
                            corr_pairs.append((var1, var2, corr_value))
                    
                    # Trier par valeur absolue de corr√©lation
                    corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)
                    
                    # Cr√©er un DataFrame pour l'affichage
                    strongest_corr = pd.DataFrame(
                        [(p[0], p[1], p[2]) for p in corr_pairs],
                        columns=["Variable 1", "Variable 2", "Corr√©lation"]
                    )
                    
                    # Styliser le DataFrame
                    def color_corr(val):
                        color = 'red' if val < 0 else 'green'
                        return f'color: {color}'
                    
                    styled_corr = strongest_corr.style.format({'Corr√©lation': '{:.4f}'}).applymap(color_corr, subset=['Corr√©lation'])
                    st.dataframe(styled_corr, use_container_width=True)
                    
                    st.markdown("</div>", unsafe_allow_html=True)
                else:
                    st.info("S√©lectionnez au moins deux variables pour afficher la matrice de corr√©lation.")
            
            elif relation_type == "Pairplot":
                # S√©lection des variables
                selected_vars = st.multiselect(
                    "S√©lectionner les variables pour le pairplot",
                    numeric_cols,
                    default=numeric_cols[:min(4, len(numeric_cols))]
                )
                
                if len(selected_vars) > 1:
                    # Cr√©er le pairplot avec plotly
                    fig = px.scatter_matrix(
                        df,
                        dimensions=selected_vars,
                        title="Matrice de nuages de points"
                    )
                    
                    # Ajustements
                    fig.update_traces(diagonal_visible=False)
                    fig.update_layout(height=800)
                    
                    st.plotly_chart(fig, use_container_width=True)
                else:
                    st.info("S√©lectionnez au moins deux variables pour afficher le pairplot.")
            
            elif relation_type == "Heatmap":
                # S√©lection des variables pour les axes
                col1, col2 = st.columns([1, 3])
                
                with col1:
                    x_var = st.selectbox("Variable X (horizontal)", numeric_cols)
                    y_var = st.selectbox("Variable Y (vertical)", numeric_cols, index=min(1, len(numeric_cols)-1))
                    z_var = st.selectbox("Variable Z (couleur)", numeric_cols, index=min(2, len(numeric_cols)-1))
                    
                    # Options
                    n_bins_x = st.slider("Nombre de bins (X)", 5, 50, 20)
                    n_bins_y = st.slider("Nombre de bins (Y)", 5, 50, 20)
                    aggregation = st.selectbox("Agr√©gation", ["mean", "median", "sum", "min", "max", "count"])
                
                with col2:
                    # Cr√©er des bins pour les variables x et y
                    df['x_bin'] = pd.cut(df[x_var], bins=n_bins_x)
                    df['y_bin'] = pd.cut(df[y_var], bins=n_bins_y)
                    
                    # Agr√©ger les donn√©es
                    heatmap_data = df.groupby(['x_bin', 'y_bin'])[z_var].agg(aggregation).reset_index()
                    
                    # Convertir les bins en points centraux pour la visualisation
                    heatmap_data['x_center'] = heatmap_data['x_bin'].apply(lambda x: x.mid)
                    heatmap_data['y_center'] = heatmap_data['y_bin'].apply(lambda x: x.mid)
                    
                    # Cr√©er un pivottage pour la heatmap
                    pivot_data = heatmap_data.pivot(index='y_center', columns='x_center', values=z_var)
                    
                    # Cr√©er la heatmap
                    fig = px.imshow(
                        pivot_data,
                        x=pivot_data.columns,
                        y=pivot_data.index,
                        color_continuous_scale="Viridis",
                        labels=dict(x=x_var, y=y_var, color=f"{z_var} ({aggregation})"),
                        title=f"Heatmap de {z_var} par {x_var} et {y_var}"
                    )
                    
                    st.plotly_chart(fig, use_container_width=True)
        
        with tab4:
            st.markdown("<h2 class='subsection-header'>Analyse de Tendances</h2>", unsafe_allow_html=True)
            
            trend_type = st.selectbox(
                "Type d'analyse",
                ["R√©gression", "Groupement", "D√©composition en composantes principales (PCA)"]
            )
            
            if trend_type == "R√©gression":
                col1, col2 = st.columns([3, 1])
                
                with col2:
                    x_var = st.selectbox("Variable ind√©pendante (X)", numeric_cols)
                    y_var = st.selectbox("Variable d√©pendante (Y)", numeric_cols, index=min(1, len(numeric_cols)-1))
                    
                    reg_type = st.selectbox("Type de r√©gression", ["lin√©aire", "polynomiale", "lowess"])
                    
                    if reg_type == "polynomiale":
                        degree = st.slider("Degr√© du polyn√¥me", 1, 10, 2)
                    
                    show_formula = st.checkbox("Afficher l'√©quation", value=True)
                    show_r2 = st.checkbox("Afficher R¬≤", value=True)
                
                with col1:
                    if reg_type == "lin√©aire":
                        fig = px.scatter(
                            df, x=x_var, y=y_var,
                            trendline="ols",
                            trendline_color_override="red",
                            labels={x_var: x_var, y_var: y_var},
                            title=f"R√©gression lin√©aire: {y_var} vs {x_var}"
                        )
                        
                        if show_formula or show_r2:
                            import statsmodels.api as sm
                            
                            X = sm.add_constant(df[x_var])
                            model = sm.OLS(df[y_var], X).fit()
                            
                            if show_formula:
                                formula = f"y = {model.params[1]:.4f}x + {model.params[0]:.4f}"
                                fig.add_annotation(
                                    x=0.05, y=0.95,
                                    xref="paper", yref="paper",
                                    text=formula,
                                    showarrow=False,
                                    font=dict(size=14),
                                    bgcolor="white",
                                    bordercolor="black",
                                    borderwidth=1
                                )
                            
                            if show_r2:
                                r2_text = f"R¬≤ = {model.rsquared:.4f}"
                                fig.add_annotation(
                                    x=0.05, y=0.85,
                                    xref="paper", yref="paper",
                                    text=r2_text,
                                    showarrow=False,
                                    font=dict(size=14),
                                    bgcolor="white",
                                    bordercolor="black",
                                    borderwidth=1
                                )
                    
                    elif reg_type == "polynomiale":
                        # Cr√©er des variables polynomiales
                        import numpy as np
                        
                        fig = px.scatter(
                            df, x=x_var, y=y_var,
                            labels={x_var: x_var, y_var: y_var},
                            title=f"R√©gression polynomiale (degr√© {degree}): {y_var} vs {x_var}"
                        )
                        
                        # Calculer la r√©gression polynomiale
                        from numpy.polynomial.polynomial import Polynomial
                        
                        x = df[x_var].values
                        y = df[y_var].values
                        
                        # Tri des points pour un tra√ßage correct de la courbe
                        sort_idx = np.argsort(x)
                        x_sorted = x[sort_idx]
                        y_sorted = y[sort_idx]
                        
                        # Ajustement polynomial
                        coeffs = np.polyfit(x, y, degree)
                        p = np.poly1d(coeffs)
                        
                        # Tracer la courbe polynomiale
                        x_range = np.linspace(min(x), max(x), 100)
                        fig.add_trace(go.Scatter(
                            x=x_range,
                            y=p(x_range),
                            mode='lines',
                            line=dict(color='red', width=2),
                            name=f'Polyn√¥me degr√© {degree}'
                        ))
                        
                        if show_formula or show_r2:
                            if show_formula:
                                formula = "y = "
                                for i, coef in enumerate(reversed(coeffs)):
                                    if i == 0:
                                        formula += f"{coef:.4f}"
                                    else:
                                        formula += f" + {coef:.4f}x^{i}"
                                
                                fig.add_annotation(
                                    x=0.05, y=0.95,
                                    xref="paper", yref="paper",
                                    text=formula,
                                    showarrow=False,
                                    font=dict(size=14),
                                    bgcolor="white",
                                    bordercolor="black",
                                    borderwidth=1
                                )
                            
                            if show_r2:
                                # Calculer R¬≤
                                y_pred = p(x)
                                r2 = r2_score(y, y_pred)
                                
                                r2_text = f"R¬≤ = {r2:.4f}"
                                fig.add_annotation(
                                    x=0.05, y=0.85,
                                    xref="paper", yref="paper",
                                    text=r2_text,
                                    showarrow=False,
                                    font=dict(size=14),
                                    bgcolor="white",
                                    bordercolor="black",
                                    borderwidth=1
                                )
                    
                    elif reg_type == "lowess":
                        fig = px.scatter(
                            df, x=x_var, y=y_var,
                            trendline="lowess",
                            trendline_color_override="red",
                            labels={x_var: x_var, y_var: y_var},
                            title=f"R√©gression LOWESS: {y_var} vs {x_var}"
                        )
                        
                        # Pas de formule pour LOWESS, mais on peut afficher une info contextuelle
                        fig.add_annotation(
                            x=0.05, y=0.95,
                            xref="paper", yref="paper",
                            text="R√©gression non param√©trique LOWESS",
                            showarrow=False,
                            font=dict(size=14),
                            bgcolor="white",
                            bordercolor="black",
                            borderwidth=1
                        )
                    
                    st.plotly_chart(fig, use_container_width=True)
                
                # Statistiques suppl√©mentaires
                with st.expander("Statistiques d√©taill√©es"):
                    if reg_type == "lin√©aire":
                        import statsmodels.api as sm
                        
                        X = sm.add_constant(df[x_var])
                        model = sm.OLS(df[y_var], X).fit()
                        
                        st.write(model.summary())
                    
                    elif reg_type == "polynomiale":
                        from sklearn.linear_model import LinearRegression
                        from sklearn.preprocessing import PolynomialFeatures
                        from sklearn.pipeline import make_pipeline
                        
                        X = df[x_var].values.reshape(-1, 1)
                        y = df[y_var].values
                        
                        model = make_pipeline(PolynomialFeatures(degree=degree), LinearRegression())
                        model.fit(X, y)
                        
                        y_pred = model.predict(X)
                        mse = mean_squared_error(y, y_pred)
                        r2 = r2_score(y, y_pred)
                        
                        st.write(f"Erreur quadratique moyenne (MSE): {mse:.4f}")
                        st.write(f"Coefficient de d√©termination (R¬≤): {r2:.4f}")
                        
                        # Coefficients du mod√®le
                        coefs = model.named_steps['linearregression'].coef_
                        intercept = model.named_steps['linearregression'].intercept_
                        
                        st.write("Coefficients du mod√®le:")
                        for i, coef in enumerate(coefs):
                            if i == 0:
                                st.write(f"Constante: {intercept:.4f}")
                            else:
                                st.write(f"x^{i}: {coef:.4f}")
            
            elif trend_type == "Groupement":
                col1, col2 = st.columns([3, 1])
                
                with col2:
                    # Variables pour le groupement
                    group_var = st.selectbox("Variable de groupement", [col for col in df.columns if col not in ['X', 'Y']])
                    agg_var = st.selectbox("Variable √† agr√©ger", numeric_cols)
                    
                    # M√©thode de groupement
                    if df[group_var].dtype.name in ['object', 'category']:
                        # Variable cat√©gorielle
                        group_method = "category"
                    else:
                        # Variable num√©rique, proposer des m√©thodes de binning
                        group_method = st.selectbox("M√©thode de groupement", ["quantiles", "equal_width", "custom"])
                        
                        if group_method == "quantiles":
                            n_groups = st.slider("Nombre de quantiles", 2, 10, 4)
                        elif group_method == "equal_width":
                            n_groups = st.slider("Nombre d'intervalles", 2, 10, 4)
                        elif group_method == "custom":
                            bin_edges = st.text_input("Limites des intervalles (s√©par√©s par des virgules)", 
                                                      value=",".join(str(round(x, 2)) for x in [df[group_var].min(), df[group_var].median(), df[group_var].max()]))
                            try:
                                bin_edges = [float(x.strip()) for x in bin_edges.split(",")]
                            except:
                                st.error("Format invalide. Utilisez des nombres s√©par√©s par des virgules.")
                                bin_edges = [df[group_var].min(), df[group_var].median(), df[group_var].max()]
                    
                    # M√©thode d'agr√©gation
                    agg_method = st.selectbox("M√©thode d'agr√©gation", ["mean", "median", "sum", "min", "max", "count", "std"])
                
                with col1:
                    # Regrouper les donn√©es selon la m√©thode choisie
                    if group_method == "category":
                        grouped = df.groupby(group_var)[agg_var].agg(agg_method).reset_index()
                        x_title = group_var
                    elif group_method == "quantiles":
                        df['group'] = pd.qcut(df[group_var], n_groups, duplicates='drop')
                        grouped = df.groupby('group')[agg_var].agg(agg_method).reset_index()
                        grouped['group_label'] = grouped['group'].apply(lambda x: f"{x.left:.2f} - {x.right:.2f}")
                        x_title = f"{group_var} (quantiles)"
                    elif group_method == "equal_width":
                        df['group'] = pd.cut(df[group_var], n_groups)
                        grouped = df.groupby('group')[agg_var].agg(agg_method).reset_index()
                        grouped['group_label'] = grouped['group'].apply(lambda x: f"{x.left:.2f} - {x.right:.2f}")
                        x_title = f"{group_var} (intervalles √©gaux)"
                    elif group_method == "custom":
                        df['group'] = pd.cut(df[group_var], bin_edges)
                        grouped = df.groupby('group')[agg_var].agg(agg_method).reset_index()
                        grouped['group_label'] = grouped['group'].apply(lambda x: f"{x.left:.2f} - {x.right:.2f}" if not pd.isna(x) else "NA")
                        x_title = f"{group_var} (intervalles personnalis√©s)"
                    
                    # Cr√©er le graphique
                    if group_method == "category":
                        fig = px.bar(
                            grouped,
                            x=group_var,
                            y=agg_var,
                            title=f"{agg_method.capitalize()} de {agg_var} par {group_var}",
                            labels={group_var: x_title, agg_var: f"{agg_method.capitalize()} de {agg_var}"}
                        )
                    else:
                        fig = px.bar(
                            grouped,
                            x='group_label',
                            y=agg_var,
                            title=f"{agg_method.capitalize()} de {agg_var} par {group_var}",
                            labels={'group_label': x_title, agg_var: f"{agg_method.capitalize()} de {agg_var}"}
                        )
                        fig.update_xaxes(tickangle=45)
                    
                    st.plotly_chart(fig, use_container_width=True)
                
                # Tableau des statistiques par groupe
                st.markdown("<div class='card'>", unsafe_allow_html=True)
                st.markdown("#### Statistiques d√©taill√©es par groupe")
                
                if group_method == "category":
                    detailed_stats = df.groupby(group_var)[agg_var].agg(['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']).reset_index()
                else:
                    detailed_stats = df.groupby('group')[agg_var].agg(['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']).reset_index()
                    detailed_stats['group'] = detailed_stats['group'].astype(str)
                
                st.dataframe(detailed_stats, use_container_width=True)
                st.markdown("</div>", unsafe_allow_html=True)
            
            elif trend_type == "D√©composition en composantes principales (PCA)":
                # S√©lection des variables pour la PCA
                selected_vars = st.multiselect(
                    "S√©lectionner les variables pour la PCA",
                    numeric_cols,
                    default=numeric_cols[:min(5, len(numeric_cols))]
                )
                
                if len(selected_vars) > 1:
                    col1, col2 = st.columns([3, 1])
                    
                    with col2:
                        n_components = st.slider("Nombre de composantes", 2, min(len(selected_vars), 10), 2)
                        scale_data = st.checkbox("Standardiser les donn√©es", value=True)
                        
                        if n_components > 2:
                            plot_type = st.selectbox("Type de visualisation", ["2D", "3D"])
                        else:
                            plot_type = "2D"
                        
                        color_by = st.selectbox("Colorer par", [None] + numeric_cols)
                    
                    with col1:
                        # Pr√©paration des donn√©es
                        X = df[selected_vars].copy()
                        
                        # Gestion des valeurs manquantes
                        if X.isnull().sum().sum() > 0:
                            X = X.fillna(X.mean())
                        
                        # Standardisation si demand√©e
                        if scale_data:
                            scaler = StandardScaler()
                            X_scaled = scaler.fit_transform(X)
                        else:
                            X_scaled = X.values
                        
                        # Calcul de la PCA
                        pca = PCA(n_components=n_components)
                        pca_result = pca.fit_transform(X_scaled)
                        
                        # Cr√©er un DataFrame avec les r√©sultats
                        pca_df = pd.DataFrame(
                            data=pca_result,
                            columns=[f'PC{i+1}' for i in range(n_components)]
                        )
                        
                        # Ajouter la variable de couleur si demand√©e
                        if color_by:
                            pca_df['color'] = df[color_by]
                        
                        # Visualisation
                        if plot_type == "2D":
                            if color_by:
                                fig = px.scatter(
                                    pca_df, x='PC1', y='PC2',
                                    color='color',
                                    title="Analyse en Composantes Principales (ACP)",
                                    labels={'color': color_by}
                                )
                            else:
                                fig = px.scatter(
                                    pca_df, x='PC1', y='PC2',
                                    title="Analyse en Composantes Principales (ACP)"
                                )
                            
                            # Ajouter des annotations pour le pourcentage de variance expliqu√©e
                            explained_variance = pca.explained_variance_ratio_
                            
                            fig.update_xaxes(title=f"PC1 ({explained_variance[0]:.1%} de variance expliqu√©e)")
                            fig.update_yaxes(title=f"PC2 ({explained_variance[1]:.1%} de variance expliqu√©e)")
                        
                        elif plot_type == "3D":
                            if color_by:
                                fig = px.scatter_3d(
                                    pca_df, x='PC1', y='PC2', z='PC3',
                                    color='color',
                                    title="Analyse en Composantes Principales (ACP) - 3D",
                                    labels={'color': color_by}
                                )
                            else:
                                fig = px.scatter_3d(
                                    pca_df, x='PC1', y='PC2', z='PC3',
                                    title="Analyse en Composantes Principales (ACP) - 3D"
                                )
                            
                            # Variance expliqu√©e
                            explained_variance = pca.explained_variance_ratio_
                            
                            fig.update_scenes(
                                xaxis_title=f"PC1 ({explained_variance[0]:.1%})",
                                yaxis_title=f"PC2 ({explained_variance[1]:.1%})",
                                zaxis_title=f"PC3 ({explained_variance[2]:.1%})"
                            )
                        
                        st.plotly_chart(fig, use_container_width=True)
                    
                    # Analyse d√©taill√©e de la PCA
                    with st.expander("Analyse d√©taill√©e de la PCA"):
                        # Variance expliqu√©e par chaque composante
                        explained_variance = pca.explained_variance_ratio_
                        cum_explained_variance = np.cumsum(explained_variance)
                        
                        # Graphique de la variance expliqu√©e
                        var_df = pd.DataFrame({
                            'Composante': [f'PC{i+1}' for i in range(n_components)],
                            'Variance Expliqu√©e (%)': explained_variance * 100,
                            'Variance Cumul√©e (%)': cum_explained_variance * 100
                        })
                        
                        fig = go.Figure()
                        
                        fig.add_trace(go.Bar(
                            x=var_df['Composante'],
                            y=var_df['Variance Expliqu√©e (%)'],
                            name='Variance Expliqu√©e'
                        ))
                        
                        fig.add_trace(go.Scatter(
                            x=var_df['Composante'],
                            y=var_df['Variance Cumul√©e (%)'],
                            mode='lines+markers',
                            name='Variance Cumul√©e',
                            line=dict(color='red')
                        ))
                        
                        fig.update_layout(
                            title="Variance expliqu√©e par composante",
                            xaxis_title="Composante",
                            yaxis_title="Variance Expliqu√©e (%)",
                            legend=dict(x=0.7, y=0.9)
                        )
                        
                        st.plotly_chart(fig, use_container_width=True)
                        
                        # Contributions des variables aux composantes
                        loadings = pca.components_
                        loading_df = pd.DataFrame(
                            loadings.T,
                            columns=[f'PC{i+1}' for i in range(n_components)],
                            index=selected_vars
                        )
                        
                        st.markdown("#### Contributions des variables aux composantes principales")
                        st.dataframe(loading_df, use_container_width=True)
                        
                        # Graphique des contributions (biplot pour les 2 premi√®res composantes)
                        st.markdown("#### Biplot (PC1 vs PC2)")
                        
                        # Standardiser les loadings pour la visualisation
                        loading_scale = 5  # Facteur d'√©chelle pour les fl√®ches
                        pcs = pca.components_
                        n = pcs.shape[1]
                        
                        # Cr√©er le biplot
                        fig = go.Figure()
                        
                        # Tracer les observations (projections)
                        fig.add_trace(go.Scatter(
                            x=pca_df['PC1'],
                            y=pca_df['PC2'],
                            mode='markers',
                            marker=dict(
                                color='blue',
                                size=8,
                                opacity=0.5
                            ),
                            name='Observations'
                        ))
                        
                        # Tracer les fl√®ches des variables
                        for i, var in enumerate(selected_vars):
                            fig.add_trace(go.Scatter(
                                x=[0, pcs[0, i] * loading_scale],
                                y=[0, pcs[1, i] * loading_scale],
                                mode='lines+markers+text',
                                line=dict(color='red', width=2),
                                marker=dict(size=5, color='red'),
                                text=['', var],
                                textposition='top center',
                                name=var
                            ))
                        
                        # Mise en page
                        fig.update_layout(
                            title="Biplot PCA",
                            xaxis=dict(
                                title=f"PC1 ({explained_variance[0]:.1%} de variance expliqu√©e)",
                                zeroline=True,
                                zerolinewidth=1,
                                zerolinecolor='black'
                            ),
                            yaxis=dict(
                                title=f"PC2 ({explained_variance[1]:.1%} de variance expliqu√©e)",
                                zeroline=True,
                                zerolinewidth=1,
                                zerolinecolor='black'
                            ),
                            showlegend=False
                        )
                        
                        st.plotly_chart(fig, use_container_width=True)
                else:
                    st.info("S√©lectionnez au moins deux variables pour effectuer une ACP.")

def show_prediction():
    st.markdown("<h1 class='title'>Pr√©diction de Min√©ralisation</h1>", unsafe_allow_html=True)
    st.markdown("<p class='subtitle'>Pr√©disez la teneur en min√©ralisation √† partir de donn√©es g√©ochimiques</p>", unsafe_allow_html=True)
    
    # Composant pour t√©l√©charger des donn√©es si n√©cessaire
    if st.session_state.data is None:
        upload_data_component()
    else:
        # Bouton pour changer de jeu de donn√©es
        if st.button("üì§ Changer de jeu de donn√©es"):
            upload_data_component()
    
    # Si des donn√©es sont charg√©es, afficher l'interface de pr√©diction
    if st.session_state.data is not None:
        df = st.session_state.data
        
        # Informations sur le jeu de donn√©es
        st.markdown(f"<div class='info-card'>Jeu de donn√©es actuel: <strong>{st.session_state.uploaded_file_name}</strong> | {len(df)} √©chantillons | {len(df.columns)} variables</div>", unsafe_allow_html=True)
        
        # Configuration du mod√®le
        st.markdown("<h2 class='section-header'>‚öôÔ∏è Configuration du Mod√®le</h2>", unsafe_allow_html=True)
        
        # Colonnes num√©riques
        numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()
        
        col1, col2 = st.columns(2)
        
        with col1:
            # S√©lection des caract√©ristiques
            features = st.multiselect(
                "S√©lectionner les caract√©ristiques (variables explicatives)",
                numeric_cols,
                default=numeric_cols[:min(3, len(numeric_cols))]
            )
        
        with col2:
            # S√©lection de la cible
            available_targets = [col for col in numeric_cols if col not in features]
            
            if available_targets:
                target = st.selectbox("S√©lectionner la variable cible √† pr√©dire", available_targets)
            else:
                st.error("Veuillez d'abord s√©lectionner des caract√©ristiques.")
                target = None
        
        if features and target:
            # Exclure la cible des caract√©ristiques si elle y est
            if target in features:
                features.remove(target)
            
            # Choix du mod√®le
            model_options = ["XGBoost", "LightGBM", "RandomForest", "GradientBoosting", "KNN"]
            
            col1, col2 = st.columns(2)
            
            with col1:
                model_type = st.selectbox("S√©lectionner le type de mod√®le", model_options)
            
            with col2:
                # Options d'√©valuation
                test_size = st.slider("Taille de l'ensemble de test (%)", 10, 50, 20) / 100
                cv_folds = st.slider("Nombre de plis pour la validation crois√©e", 2, 10, 5)
            
            # Param√®tres avanc√©s par type de mod√®le
            with st.expander("Param√®tres avanc√©s du mod√®le"):
                if model_type == "XGBoost":
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        n_estimators = st.slider("Nombre d'estimateurs", 50, 1000, 100)
                        learning_rate = st.slider("Taux d'apprentissage", 0.01, 0.3, 0.1, 0.01)
                    
                    with col2:
                        max_depth = st.slider("Profondeur maximale", 3, 15, 6)
                        gamma = st.slider("Gamma", 0.0, 1.0, 0.0, 0.1)
                    
                    with col3:
                        subsample = st.slider("Subsample", 0.5, 1.0, 1.0, 0.1)
                        colsample_bytree = st.slider("Colsample Bytree", 0.5, 1.0, 1.0, 0.1)
                    
                    model_params = {
                        "n_estimators": n_estimators,
                        "learning_rate": learning_rate,
                        "max_depth": max_depth,
                        "gamma": gamma,
                        "subsample": subsample,
                        "colsample_bytree": colsample_bytree,
                        "random_state": 42
                    }
                
                elif model_type == "LightGBM":
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        n_estimators = st.slider("Nombre d'estimateurs", 50, 1000, 100)
                        learning_rate = st.slider("Taux d'apprentissage", 0.01, 0.3, 0.1, 0.01)
                    
                    with col2:
                        max_depth = st.slider("Profondeur maximale", 3, 15, 6)
                        num_leaves = st.slider("Nombre de feuilles", 10, 100, 31)
                    
                    with col3:
                        subsample = st.slider("Subsample", 0.5, 1.0, 1.0, 0.1)
                        colsample_bytree = st.slider("Colsample Bytree", 0.5, 1.0, 1.0, 0.1)
                    
                    model_params = {
                        "n_estimators": n_estimators,
                        "learning_rate": learning_rate,
                        "max_depth": max_depth,
                        "num_leaves": num_leaves,
                        "subsample": subsample,
                        "colsample_bytree": colsample_bytree,
                        "random_state": 42
                    }
                
                elif model_type == "RandomForest":
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        n_estimators = st.slider("Nombre d'estimateurs", 50, 1000, 100)
                        max_depth = st.slider("Profondeur maximale", 3, 15, 6)
                    
                    with col2:
                        min_samples_split = st.slider("√âchantillons minimum pour la division", 2, 20, 2)
                        min_samples_leaf = st.slider("√âchantillons minimum par feuille", 1, 20, 1)
                    
                    with col3:
                        max_features = st.selectbox("Caract√©ristiques maximum", ["auto", "sqrt", "log2"])
                        bootstrap = st.checkbox("Bootstrap", value=True)
                    
                    model_params = {
                        "n_estimators": n_estimators,
                        "max_depth": max_depth,
                        "min_samples_split": min_samples_split,
                        "min_samples_leaf": min_samples_leaf,
                        "max_features": max_features,
                        "bootstrap": bootstrap,
                        "random_state": 42
                    }
                
                elif model_type == "GradientBoosting":
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        n_estimators = st.slider("Nombre d'estimateurs", 50, 1000, 100)
                        learning_rate = st.slider("Taux d'apprentissage", 0.01, 0.3, 0.1, 0.01)
                    
                    with col2:
                        max_depth = st.slider("Profondeur maximale", 3, 15, 3)
                        min_samples_split = st.slider("√âchantillons minimum pour la division", 2, 20, 2)
                    
                    with col3:
                        min_samples_leaf = st.slider("√âchantillons minimum par feuille", 1, 20, 1)
                        subsample = st.slider("Subsample", 0.5, 1.0, 1.0, 0.1)
                    
                    model_params = {
                        "n_estimators": n_estimators,
                        "learning_rate": learning_rate,
                        "max_depth": max_depth,
                        "min_samples_split": min_samples_split,
                        "min_samples_leaf": min_samples_leaf,
                        "subsample": subsample,
                        "random_state": 42
                    }
                
                elif model_type == "KNN":
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        n_neighbors = st.slider("Nombre de voisins", 1, 20, 5)
                        weights = st.selectbox("Pond√©ration", ["uniform", "distance"])
                    
                    with col2:
                        algorithm = st.selectbox("Algorithme", ["auto", "ball_tree", "kd_tree", "brute"])
                        leaf_size = st.slider("Taille des feuilles", 10, 100, 30)
                    
                    model_params = {
                        "n_neighbors": n_neighbors,
                        "weights": weights,
                        "algorithm": algorithm,
                        "leaf_size": leaf_size
                    }
            
            # Entra√Ænement du mod√®le
            train_button = st.button("üî• Entra√Æner le mod√®le")
            
            if train_button:
                # Pr√©paration des donn√©es
                X = df[features].copy()
                y = df[target].copy()
                
                # Gestion des valeurs manquantes
                if X.isnull().sum().sum() > 0 or y.isnull().sum() > 0:
                    st.warning("Des valeurs manquantes ont √©t√© d√©tect√©es. Elles seront remplac√©es par la m√©diane.")
                    X = X.fillna(X.median())
                    y = y.fillna(y.median())
                
                # Division train/test
                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)
                
                # Standardisation des donn√©es
                scaler = StandardScaler()
                X_train_scaled = scaler.fit_transform(X_train)
                X_test_scaled = scaler.transform(X_test)
                
                # Affichage d'une barre de progression pendant l'entra√Ænement
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                # √âtape 1: Entra√Ænement du mod√®le
                status_text.text("Entra√Ænement du mod√®le en cours...")
                progress_bar.progress(10)
                
                # Entra√Æner le mod√®le
                model = train_model(X_train, y_train, model_type, model_params)
                progress_bar.progress(40)
                
                # √âtape 2: √âvaluation sur l'ensemble de test
                status_text.text("√âvaluation du mod√®le sur l'ensemble de test...")
                progress_bar.progress(60)
                
                # √âvaluer le mod√®le
                test_metrics = evaluate_model(model, X_test, y_test)
                progress_bar.progress(80)
                
                # √âtape 3: Validation crois√©e
                status_text.text("Validation crois√©e en cours...")
                cv_metrics = cross_validate_model(model, X, y, cv=cv_folds)
                progress_bar.progress(100)
                
                # Effacer la barre de progression et le texte de statut
                progress_bar.empty()
                status_text.empty()
                
                # Sauvegarder le mod√®le en session
                st.session_state.model = model
                st.session_state.model_features = features
                st.session_state.model_target = target
                st.session_state.model_metrics = {
                    "test": test_metrics,
                    "cv": cv_metrics
                }
                
                # Afficher un message de succ√®s
                st.success("‚úÖ Mod√®le entra√Æn√© avec succ√®s!")
            
            # Afficher les r√©sultats si un mod√®le a √©t√© entra√Æn√©
            if st.session_state.model is not None and st.session_state.model_features == features and st.session_state.model_target == target:
                st.markdown("<h2 class='section-header'>üìä R√©sultats du Mod√®le</h2>", unsafe_allow_html=True)
                
                # R√©cup√©rer les m√©triques
                test_metrics = st.session_state.model_metrics["test"]
                cv_metrics = st.session_state.model_metrics["cv"]
                
                # Afficher les m√©triques principales
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.markdown("<div class='metric-card'>", unsafe_allow_html=True)
                    st.markdown(f"<div class='metric-value'>{test_metrics['R¬≤']:.4f}</div>", unsafe_allow_html=True)
                    st.markdown("<div class='metric-label'>R¬≤ (Test)</div>", unsafe_allow_html=True)
                    st.markdown("</div>", unsafe_allow_html=True)
                
                with col2:
                    st.markdown("<div class='metric-card'>", unsafe_allow_html=True)
                    st.markdown(f"<div class='metric-value'>{test_metrics['RMSE']:.4f}</div>", unsafe_allow_html=True)
                    st.markdown("<div class='metric-label'>RMSE (Test)</div>", unsafe_allow_html=True)
                    st.markdown("</div>", unsafe_allow_html=True)
                
                with col3:
                    st.markdown("<div class='metric-card'>", unsafe_allow_html=True)
                    st.markdown(f"<div class='metric-value'>{cv_metrics['CV R¬≤']:.4f}</div>", unsafe_allow_html=True)
                    st.markdown("<div class='metric-label'>R¬≤ (CV)</div>", unsafe_allow_html=True)
                    st.markdown("</div>", unsafe_allow_html=True)
                
                with col4:
                    st.markdown("<div class='metric-card'>", unsafe_allow_html=True)
                    st.markdown(f"<div class='metric-value'>{cv_metrics['CV RMSE']:.4f}</div>", unsafe_allow_html=True)
                    st.markdown("<div class='metric-label'>RMSE (CV)</div>", unsafe_allow_html=True)
                    st.markdown("</div>", unsafe_allow_html=True)
                
                # Visualisations des r√©sultats
                st.markdown("<h3 class='subsection-header'>Visualisations</h3>", unsafe_allow_html=True)
                
                tab1, tab2, tab3 = st.tabs(["Pr√©dictions vs R√©alit√©", "Importance des Variables", "Carte de Pr√©diction"])
                
                with tab1:
                    # Scatter plot des pr√©dictions vs r√©alit√©
                    X_test = df[features].sample(frac=test_size, random_state=42)
                    y_test = df[target].loc[X_test.index]
                    y_pred = test_metrics["Pr√©dictions"]
                    
                    fig = px.scatter(
                        x=y_test,
                        y=y_pred,
                        trendline="ols",
                        trendline_color_override="red",
                        labels={"x": f"{target} r√©el", "y": f"{target} pr√©dit"},
                        title="Pr√©dictions vs Valeurs r√©elles"
                    )
                    
                    # Ligne y=x (pr√©diction parfaite)
                    fig.add_trace(go.Scatter(
                        x=[y_test.min(), y_test.max()],
                        y=[y_test.min(), y_test.max()],
                        mode="lines",
                        line=dict(color="black", dash="dash"),
                        name="Pr√©diction parfaite"
                    ))
                    
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # Histogramme des erreurs
                    errors = y_pred - y_test
                    
                    fig = px.histogram(
                        errors,
                        nbins=20,
                        marginal="box",
                        title="Distribution des erreurs de pr√©diction",
                        color_discrete_sequence=['#FF7043']
                    )
                    
                    fig.add_vline(x=0, line_dash="dash", line_color="red")
                    
                    st.plotly_chart(fig, use_container_width=True)
                
                with tab2:
                    # Importance des variables
                    if model_type in ["XGBoost", "LightGBM", "RandomForest", "GradientBoosting"]:
                        importances = st.session_state.model.feature_importances_
                        importance_df = pd.DataFrame({
                            'Feature': features,
                            'Importance': importances
                        }).sort_values(by='Importance', ascending=False)
                        
                        fig = px.bar(
                            importance_df,
                            x='Importance',
                            y='Feature',
                            orientation='h',
                            title="Importance des Variables",
                            color='Importance',
                            color_continuous_scale='Blues'
                        )
                        
                        st.plotly_chart(fig, use_container_width=True)
                        
                        # Table d'importance des variables
                        st.dataframe(importance_df.style.bar(subset=['Importance'], color='#71a4f9'), use_container_width=True)
                    else:
                        st.info("L'importance des variables n'est pas disponible pour ce type de mod√®le.")
                
                with tab3:
                    # Carte de pr√©diction si les coordonn√©es sont disponibles
                    if 'X' in df.columns and 'Y' in df.columns:
                        # Pr√©dire sur l'ensemble complet des donn√©es
                        X_full = df[features]
                        predictions = st.session_state.model.predict(X_full)
                        
                        # Ajouter les pr√©dictions au DataFrame
                        result_df = df.copy()
                        result_df[f'{target}_pr√©dit'] = predictions
                        result_df[f'Erreur_{target}'] = result_df[f'{target}_pr√©dit'] - result_df[target]
                        
                        # S√©lection du type de carte
                        map_type = st.selectbox(
                            "Type de visualisation",
                            ["Valeurs pr√©dites", "Erreurs de pr√©diction", "Comparaison pr√©dictions vs r√©alit√©"]
                        )
                        
                        if map_type == "Valeurs pr√©dites":
                            # Carte des valeurs pr√©dites
                            fig = px.scatter(
                                result_df,
                                x='X', y='Y',
                                color=f'{target}_pr√©dit',
                                size=f'{target}_pr√©dit',
                                color_continuous_scale='Viridis',
                                title=f"Carte des valeurs pr√©dites de {target}"
                            )
                            
                            st.plotly_chart(fig, use_container_width=True)
                            
                            # Carte de contour
                            with st.expander("Carte de contour des pr√©dictions"):
                                interp_data = interpolate_values(result_df, f'{target}_pr√©dit', grid_size=100, method='linear')
                                
                                if interp_data:
                                    fig = plot_contour_map(interp_data, result_df, f'{target}_pr√©dit', colorscale='Viridis')
                                    st.plotly_chart(fig, use_container_width=True)
                        
                        elif map_type == "Erreurs de pr√©diction":
                            # Carte des erreurs de pr√©diction
                            fig = px.scatter(
                                result_df,
                                x='X', y='Y',
                                color=f'Erreur_{target}',
                                size=abs(result_df[f'Erreur_{target}']),
                                color_continuous_scale='RdBu_r',
                                title=f"Carte des erreurs de pr√©diction de {target}"
                            )
                            
                            # Ajuster l'√©chelle de couleurs pour √™tre centr√©e sur 0
                            max_abs_error = max(abs(result_df[f'Erreur_{target}'].max()), abs(result_df[f'Erreur_{target}'].min()))
                            fig.update_layout(coloraxis_colorbar=dict(title="Erreur"))
                            fig.update_traces(marker=dict(cmin=-max_abs_error, cmax=max_abs_error, colorbar=dict(title="Erreur")))
                            
                            st.plotly_chart(fig, use_container_width=True)
                        
                        elif map_type == "Comparaison pr√©dictions vs r√©alit√©":
                            # Deux cartes c√¥te √† c√¥te: r√©alit√© et pr√©dictions
                            col1, col2 = st.columns(2)
                            
                            with col1:
                                fig1 = px.scatter(
                                    result_df,
                                    x='X', y='Y',
                                    color=target,
                                    title=f"{target} (Valeurs r√©elles)",
                                    color_continuous_scale='Viridis'
                                )
                                
                                st.plotly_chart(fig1, use_container_width=True)
                            
                            with col2:
                                fig2 = px.scatter(
                                    result_df,
                                    x='X', y='Y',
                                    color=f'{target}_pr√©dit',
                                    title=f"{target} (Valeurs pr√©dites)",
                                    color_continuous_scale='Viridis'
                                )
                                
                                st.plotly_chart(fig2, use_container_width=True)
                    else:
                        st.info("Les colonnes X et Y sont n√©cessaires pour afficher la carte de pr√©diction.")
                
                # T√©l√©chargement des r√©sultats
                st.markdown("<h3 class='subsection-header'>T√©l√©chargement des R√©sultats</h3>", unsafe_allow_html=True)
                
                # Pr√©dire sur l'ensemble complet des donn√©es
                X_full = df[features]
                predictions = st.session_state.model.predict(X_full)
                
                # Ajouter les pr√©dictions au DataFrame
                result_df = df.copy()
                result_df[f'{target}_pr√©dit'] = predictions
                result_df[f'Erreur_{target}'] = result_df[f'{target}_pr√©dit'] - result_df[target]
                
                # Afficher un aper√ßu des r√©sultats
                st.dataframe(result_df, use_container_width=True)
                
                # Bouton de t√©l√©chargement
                csv = result_df.to_csv(index=False)
                st.download_button(
                    label="üì• T√©l√©charger les r√©sultats (CSV)",
                    data=csv,
                    file_name=f"resultats_prediction_{target}.csv",
                    mime="text/csv"
                )
        else:
            st.warning("Veuillez s√©lectionner au moins une caract√©ristique et une variable cible.")

def show_anomaly_detection():
    st.markdown("<h1 class='title'>D√©tection d'Anomalies</h1>", unsafe_allow_html=True)
    st.markdown("<p class='subtitle'>Identifiez les anomalies g√©ochimiques dans vos donn√©es</p>", unsafe_allow_html=True)
    
    # Composant pour t√©l√©charger des donn√©es si n√©cessaire
    if st.session_state.data is None:
        upload_data_component()
    else:
        # Bouton pour changer de jeu de donn√©es
        if st.button("üì§ Changer de jeu de donn√©es"):
            upload_data_component()
    
    # Si des donn√©es sont charg√©es, afficher l'interface de d√©tection d'anomalies
    if st.session_state.data is not None:
        df = st.session_state.data
        
        # Informations sur le jeu de donn√©es
        st.markdown(f"<div class='info-card'>Jeu de donn√©es actuel: <strong>{st.session_state.uploaded_file_name}</strong> | {len(df)} √©chantillons | {len(df.columns)} variables</div>", unsafe_allow_html=True)
        
        # Configuration de la d√©tection d'anomalies
        st.markdown("<h2 class='section-header'>‚öôÔ∏è Configuration de la D√©tection d'Anomalies</h2>", unsafe_allow_html=True)
        
        # Colonnes num√©riques
        numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()
        
        # S√©lection des variables
        variables = st.multiselect(
            "S√©lectionner les variables pour la d√©tection d'anomalies",
            numeric_cols,
            default=numeric_cols[:min(5, len(numeric_cols))]
        )
        
        if variables:
            col1, col2, col3 = st.columns(3)
            
            with col1:
                # M√©thode de d√©tection
                method = st.selectbox(
                    "M√©thode de d√©tection",
                    ["IsolationForest", "DBSCAN", "KMeans"]
                )
            
            with col2:
                # Param√®tres communs
                if method == "IsolationForest":
                    contamination = st.slider("Contamination estim√©e (%)", 1, 20, 10) / 100
                    scaling = st.checkbox("Standardiser les donn√©es", value=True)
                elif method == "DBSCAN":
                    eps = st.slider("Epsilon (distance maximum)", 0.1, 2.0, 0.5, 0.1)
                    min_samples = st.slider("Nombre minimum d'√©chantillons", 2, 10, 5)
                    scaling = st.checkbox("Standardiser les donn√©es", value=True)
                elif method == "KMeans":
                    n_clusters = st.slider("Nombre de clusters", 2, 10, 5)
                    contamination = st.slider("Contamination estim√©e (%)", 1, 20, 10) / 100
                    scaling = st.checkbox("Standardiser les donn√©es", value=True)
            
            with col3:
                # Param√®tres additionnels
                if method == "IsolationForest":
                    n_estimators = st.slider("Nombre d'estimateurs", 50, 200, 100)
                    max_samples = st.selectbox("Taille maximum d'√©chantillons", ["auto", 100, 0.5, 0.1])
                    if max_samples == "auto":
                        max_samples_param = "auto"
                    elif max_samples == 0.5 or max_samples == 0.1:
                        max_samples_param = float(max_samples)
                    else:
                        max_samples_param = int(max_samples)
                elif method == "DBSCAN":
                    algorithm = st.selectbox("Algorithme", ["auto", "ball_tree", "kd_tree", "brute"])
                    leaf_size = st.slider("Taille des feuilles", 10, 100, 30)
                elif method == "KMeans":
                    init = st.selectbox("Initialisation", ["k-means++", "random"])
                    n_init = st.slider("Nombre d'initialisations", 1, 20, 10)
            
            # Bouton pour lancer la d√©tection
            detect_button = st.button("üîç D√©tecter les anomalies")
            
            if detect_button:
                # Pr√©paration des donn√©es
                X = df[variables].copy()
                
                # Gestion des valeurs manquantes
                if X.isnull().sum().sum() > 0:
                    st.warning("Des valeurs manquantes ont √©t√© d√©tect√©es. Elles seront remplac√©es par la m√©diane.")
                    X = X.fillna(X.median())
                
                # Standardisation des donn√©es si demand√©e
                if scaling:
                    scaler = StandardScaler()
                    X_scaled = scaler.fit_transform(X)
                else:
                    X_scaled = X.values
                
                # Affichage d'une barre de progression
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                # D√©tection des anomalies
                status_text.text("D√©tection d'anomalies en cours...")
                progress_bar.progress(30)
                
                # Param√®tres sp√©cifiques √† la m√©thode
                if method == "IsolationForest":
                    kwargs = {
                        "n_estimators": n_estimators,
                        "max_samples": max_samples_param,
                        "random_state": 42
                    }
                    anomalies, anomaly_scores = detect_anomalies(X_scaled, contamination, method, **kwargs)
                
                elif method == "DBSCAN":
                    kwargs = {
                        "eps": eps,
                        "min_samples": min_samples,
                        "algorithm": algorithm,
                        "leaf_size": leaf_size
                    }
                    anomalies, anomaly_scores = detect_anomalies(X_scaled, None, method, **kwargs)
                
                elif method == "KMeans":
                    kwargs = {
                        "n_clusters": n_clusters,
                        "init": init,
                        "n_init": n_init,
                        "random_state": 42
                    }
                    anomalies, anomaly_scores = detect_anomalies(X_scaled, contamination, method,
                   elif method == "KMeans":
                    kwargs = {
                        "n_clusters": n_clusters,
                        "init": init,
                        "n_init": n_init,
                        "random_state": 42
                    }
                    anomalies, anomaly_scores = detect_anomalies(X_scaled, contamination, method, **kwargs)
                
                progress_bar.progress(70)
                
                # Ajout des r√©sultats au dataframe
                result_df = df.copy()
                result_df["Anomalie"] = anomalies
                result_df["Score_Anomalie"] = anomaly_scores
                
                progress_bar.progress(100)
                progress_bar.empty()
                status_text.empty()
                
                # Afficher les r√©sultats
                st.success(f"‚úÖ D√©tection d'anomalies termin√©e! {np.sum(anomalies)} anomalies d√©tect√©es ({np.sum(anomalies) / len(df) * 100:.1f}%).")
                
                # Onglets pour diff√©rentes visualisations des r√©sultats
                st.markdown("<h2 class='section-header'>üìä R√©sultats</h2>", unsafe_allow_html=True)
                
                # Affichage des statistiques principales
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.markdown("<div class='metric-card'>", unsafe_allow_html=True)
                    st.markdown(f"<div class='metric-value'>{np.sum(anomalies)}</div>", unsafe_allow_html=True)
                    st.markdown("<div class='metric-label'>Anomalies d√©tect√©es</div>", unsafe_allow_html=True)
                    st.markdown("</div>", unsafe_allow_html=True)
                
                with col2:
                    st.markdown("<div class='metric-card'>", unsafe_allow_html=True)
                    st.markdown(f"<div class='metric-value'>{np.sum(anomalies) / len(df) * 100:.1f}%</div>", unsafe_allow_html=True)
                    st.markdown("<div class='metric-label'>Pourcentage d'anomalies</div>", unsafe_allow_html=True)
                    st.markdown("</div>", unsafe_allow_html=True)
                
                with col3:
                    # Score moyen des anomalies
                    if np.sum(anomalies) > 0:
                        avg_score = np.mean(anomaly_scores[anomalies == 1])
                    else:
                        avg_score = 0
                    
                    st.markdown("<div class='metric-card'>", unsafe_allow_html=True)
                    st.markdown(f"<div class='metric-value'>{avg_score:.3f}</div>", unsafe_allow_html=True)
                    st.markdown("<div class='metric-label'>Score moyen des anomalies</div>", unsafe_allow_html=True)
                    st.markdown("</div>", unsafe_allow_html=True)
                
                # Onglets pour les diff√©rentes visualisations
                tab1, tab2, tab3, tab4 = st.tabs(["Tableau de donn√©es", "Visualisations 2D/3D", "Carte des anomalies", "Statistiques comparatives"])
                
                with tab1:
                    # Option pour filtrer les anomalies
                    show_only_anomalies = st.checkbox("Afficher uniquement les anomalies", value=False)
                    
                    if show_only_anomalies:
                        filtered_df = result_df[result_df["Anomalie"] == 1]
                    else:
                        filtered_df = result_df
                    
                    # Tri des donn√©es par score d'anomalie
                    sorted_df = filtered_df.sort_values(by="Score_Anomalie", ascending=False)
                    
                    # Affichage du tableau
                    st.markdown("<div class='dataframe-container'>", unsafe_allow_html=True)
                    st.dataframe(sorted_df, use_container_width=True)
                    st.markdown("</div>", unsafe_allow_html=True)
                    
                    # T√©l√©chargement des r√©sultats
                    csv = result_df.to_csv(index=False)
                    st.download_button(
                        label="üì• T√©l√©charger les r√©sultats (CSV)",
                        data=csv,
                        file_name="resultats_anomalies.csv",
                        mime="text/csv"
                    )
                
                with tab2:
                    # S√©lection de visualisation
                    viz_type = st.radio("Type de visualisation", ["2D (PCA)", "3D (PCA)"], horizontal=True)
                    
                    # Appliquer PCA pour la visualisation
                    if len(variables) >= 2:
                        n_components = min(3, len(variables))
                        pca = PCA(n_components=n_components)
                        pca_result = pca.fit_transform(X_scaled)
                        
                        # Cr√©er un dataframe pour la visualisation
                        pca_df = pd.DataFrame(data=pca_result, columns=[f"PC{i+1}" for i in range(n_components)])
                        pca_df["Anomalie"] = anomalies
                        pca_df["Score_Anomalie"] = anomaly_scores
                        
                        # Variance expliqu√©e
                        explained_var = pca.explained_variance_ratio_
                        
                        if viz_type == "2D (PCA)":
                            fig = px.scatter(
                                pca_df,
                                x="PC1",
                                y="PC2",
                                color="Anomalie",
                                size="Score_Anomalie",
                                hover_data=["Score_Anomalie"],
                                color_discrete_sequence=["#1E88E5", "#FF5252"],
                                title="Visualisation 2D des anomalies (PCA)"
                            )
                            
                            fig.update_layout(
                                xaxis_title=f"PC1 ({explained_var[0]:.1%} variance expliqu√©e)",
                                yaxis_title=f"PC2 ({explained_var[1]:.1%} variance expliqu√©e)"
                            )
                            
                            st.plotly_chart(fig, use_container_width=True)
                            
                            # Histogramme des scores d'anomalie
                            fig = px.histogram(
                                pca_df,
                                x="Score_Anomalie",
                                color="Anomalie",
                                title="Distribution des scores d'anomalie",
                                color_discrete_sequence=["#1E88E5", "#FF5252"],
                                nbins=50
                            )
                            
                            st.plotly_chart(fig, use_container_width=True)
                        
                        elif viz_type == "3D (PCA)" and n_components >= 3:
                            fig = px.scatter_3d(
                                pca_df,
                                x="PC1",
                                y="PC2",
                                z="PC3",
                                color="Anomalie",
                                size="Score_Anomalie",
                                hover_data=["Score_Anomalie"],
                                color_discrete_sequence=["#1E88E5", "#FF5252"],
                                title="Visualisation 3D des anomalies (PCA)"
                            )
                            
                            fig.update_layout(
                                scene=dict(
                                    xaxis_title=f"PC1 ({explained_var[0]:.1%})",
                                    yaxis_title=f"PC2 ({explained_var[1]:.1%})",
                                    zaxis_title=f"PC3 ({explained_var[2]:.1%})"
                                )
                            )
                            
                            st.plotly_chart(fig, use_container_width=True)
                    else:
                        st.warning("Au moins deux variables sont n√©cessaires pour la visualisation PCA.")
                
                with tab3:
                    # Carte des anomalies
                    if 'X' in df.columns and 'Y' in df.columns:
                        # S√©lection du type de carte
                        map_type = st.selectbox(
                            "Type de carte",
                            ["Carte des points", "Carte de chaleur", "Carte de contour"]
                        )
                        
                        if map_type == "Carte des points":
                            fig = px.scatter(
                                result_df,
                                x="X",
                                y="Y",
                                color="Anomalie",
                                size="Score_Anomalie",
                                hover_data=variables + ["Score_Anomalie"],
                                color_discrete_sequence=["#1E88E5", "#FF5252"],
                                title="Carte des anomalies"
                            )
                            
                            st.plotly_chart(fig, use_container_width=True)
                        
                        elif map_type == "Carte de chaleur":
                            # Cr√©er une grille r√©guli√®re
                            x_range = np.linspace(result_df['X'].min(), result_df['X'].max(), 100)
                            y_range = np.linspace(result_df['Y'].min(), result_df['Y'].max(), 100)
                            xx, yy = np.meshgrid(x_range, y_range)
                            
                            # Interpolation des scores d'anomalie
                            from scipy.interpolate import griddata
                            z = griddata((result_df['X'], result_df['Y']), result_df["Score_Anomalie"], (xx, yy), method='linear')
                            
                            # Cr√©er la carte de chaleur
                            fig = go.Figure(data=go.Heatmap(
                                z=z,
                                x=x_range,
                                y=y_range,
                                colorscale="Viridis",
                                colorbar=dict(title="Score d'anomalie")
                            ))
                            
                            # Ajouter les points des anomalies
                            anomalies_df = result_df[result_df["Anomalie"] == 1]
                            
                            fig.add_trace(go.Scatter(
                                x=anomalies_df['X'],
                                y=anomalies_df['Y'],
                                mode='markers',
                                marker=dict(
                                    color='red',
                                    size=8,
                                    symbol='x',
                                    line=dict(color='black', width=1)
                                ),
                                name='Anomalies'
                            ))
                            
                            fig.update_layout(
                                title="Carte de chaleur des scores d'anomalie",
                                xaxis_title='X',
                                yaxis_title='Y'
                            )
                            
                            st.plotly_chart(fig, use_container_width=True)
                        
                        elif map_type == "Carte de contour":
                            # Interpolation des scores d'anomalie
                            interp_data = interpolate_values(result_df, "Score_Anomalie", grid_size=100, method='linear')
                            
                            if interp_data:
                                # Cr√©er la figure de base
                                fig = go.Figure()
                                
                                # Ajouter le contour
                                fig.add_trace(go.Contour(
                                    z=interp_data["z"],
                                    x=interp_data["x"],
                                    y=interp_data["y"],
                                    colorscale="Viridis",
                                    colorbar=dict(title="Score d'anomalie"),
                                    line=dict(width=0.5),
                                    contours=dict(
                                        showlabels=True,
                                        labelfont=dict(size=10, color='white')
                                    )
                                ))
                                
                                # Ajouter tous les points d'√©chantillonnage
                                fig.add_trace(go.Scatter(
                                    x=result_df[result_df["Anomalie"] == 0]['X'],
                                    y=result_df[result_df["Anomalie"] == 0]['Y'],
                                    mode='markers',
                                    marker=dict(
                                        color='blue',
                                        size=6,
                                        opacity=0.5
                                    ),
                                    name='Points normaux'
                                ))
                                
                                # Ajouter les anomalies
                                fig.add_trace(go.Scatter(
                                    x=result_df[result_df["Anomalie"] == 1]['X'],
                                    y=result_df[result_df["Anomalie"] == 1]['Y'],
                                    mode='markers',
                                    marker=dict(
                                        color='red',
                                        size=8,
                                        symbol='x',
                                        line=dict(color='black', width=1)
                                    ),
                                    name='Anomalies'
                                ))
                                
                                # Mise en page
                                fig.update_layout(
                                    title="Carte de contour des scores d'anomalie",
                                    xaxis_title='X',
                                    yaxis_title='Y'
                                )
                                
                                st.plotly_chart(fig, use_container_width=True)
                    else:
                        st.warning("Les colonnes X et Y sont n√©cessaires pour afficher la carte des anomalies.")
                
                with tab4:
                    # Statistiques comparatives
                    st.markdown("<h3 class='subsection-header'>Comparaison des statistiques</h3>", unsafe_allow_html=True)
                    
                    # Diviser le dataframe en deux: normal et anomalies
                    normal_df = result_df[result_df["Anomalie"] == 0]
                    anomaly_df = result_df[result_df["Anomalie"] == 1]
                    
                    # Calculer les statistiques pour chaque groupe
                    stats_columns = variables
                    
                    # Cr√©er un tableau de comparaison
                    comparison_data = []
                    
                    for col in stats_columns:
                        normal_mean = normal_df[col].mean()
                        normal_std = normal_df[col].std()
                        normal_min = normal_df[col].min()
                        normal_max = normal_df[col].max()
                        
                        if len(anomaly_df) > 0:
                            anomaly_mean = anomaly_df[col].mean()
                            anomaly_std = anomaly_df[col].std()
                            anomaly_min = anomaly_df[col].min()
                            anomaly_max = anomaly_df[col].max()
                            
                            # Diff√©rence relative en pourcentage
                            if normal_mean != 0:
                                diff_percent = (anomaly_mean - normal_mean) / abs(normal_mean) * 100
                            else:
                                diff_percent = float('inf') if anomaly_mean > 0 else float('-inf') if anomaly_mean < 0 else 0
                        else:
                            anomaly_mean = np.nan
                            anomaly_std = np.nan
                            anomaly_min = np.nan
                            anomaly_max = np.nan
                            diff_percent = np.nan
                        
                        comparison_data.append({
                            "Variable": col,
                            "Moyenne (Normal)": normal_mean,
                            "√âcart-type (Normal)": normal_std,
                            "Min (Normal)": normal_min,
                            "Max (Normal)": normal_max,
                            "Moyenne (Anomalie)": anomaly_mean,
                            "√âcart-type (Anomalie)": anomaly_std,
                            "Min (Anomalie)": anomaly_min,
                            "Max (Anomalie)": anomaly_max,
                            "Diff√©rence (%)": diff_percent
                        })
                    
                    # Cr√©er le dataframe
                    comparison_df = pd.DataFrame(comparison_data)
                    
                    # Afficher le tableau de comparaison
                    st.markdown("<div class='dataframe-container'>", unsafe_allow_html=True)
                    
                    # Formater les nombres pour une meilleure lisibilit√©
                    formatted_df = comparison_df.copy()
                    for col in formatted_df.columns:
                        if col != "Variable":
                            formatted_df[col] = formatted_df[col].map(lambda x: f"{x:.2f}" if not pd.isna(x) else "-")
                    
                    st.dataframe(formatted_df, use_container_width=True)
                    st.markdown("</div>", unsafe_allow_html=True)
                    
                    # Visualisations comparatives
                    st.markdown("<h3 class='subsection-header'>Visualisations comparatives</h3>", unsafe_allow_html=True)
                    
                    # S√©lectionner une variable pour la visualisation
                    selected_var = st.selectbox("S√©lectionner une variable pour la comparaison", variables)
                    
                    # Cr√©er des histogrammes comparatifs
                    fig = go.Figure()
                    
                    fig.add_trace(go.Histogram(
                        x=normal_df[selected_var],
                        name='Normal',
                        opacity=0.7,
                        marker_color='#1E88E5'
                    ))
                    
                    if len(anomaly_df) > 0:
                        fig.add_trace(go.Histogram(
                            x=anomaly_df[selected_var],
                            name='Anomalie',
                            opacity=0.7,
                            marker_color='#FF5252'
                        ))
                    
                    fig.update_layout(
                        title=f"Distribution de {selected_var} par groupe",
                        xaxis_title=selected_var,
                        yaxis_title='Fr√©quence',
                        barmode='overlay'
                    )
                    
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # Boxplots comparatifs pour toutes les variables
                    st.markdown("<h3 class='subsection-header'>Boxplots comparatifs</h3>", unsafe_allow_html=True)
                    
                    # Pr√©parer les donn√©es pour les boxplots
                    boxplot_data = []
                    
                    for var in variables:
                        for val in normal_df[var]:
                            boxplot_data.append({
                                "Variable": var,
                                "Valeur": val,
                                "Groupe": "Normal"
                            })
                        
                        if len(anomaly_df) > 0:
                            for val in anomaly_df[var]:
                                boxplot_data.append({
                                    "Variable": var,
                                    "Valeur": val,
                                    "Groupe": "Anomalie"
                                })
                    
                    boxplot_df = pd.DataFrame(boxplot_data)
                    
                    # Cr√©er les boxplots
                    fig = px.box(
                        boxplot_df,
                        x="Variable",
                        y="Valeur",
                        color="Groupe",
                        title="Comparaison des distributions par groupe",
                        color_discrete_map={"Normal": "#1E88E5", "Anomalie": "#FF5252"}
                    )
                    
                    fig.update_layout(
                        xaxis_title="",
                        yaxis_title="Valeur",
                        boxmode="group"
                    )
                    
                    st.plotly_chart(fig, use_container_width=True)
        else:
            st.warning("Veuillez s√©lectionner au moins une variable pour la d√©tection d'anomalies.")

def show_recommendation():
    st.markdown("<h1 class='title'>Recommandation de Cibles</h1>", unsafe_allow_html=True)
    st.markdown("<p class='subtitle'>Identifiez les emplacements optimaux pour de futurs pr√©l√®vements</p>", unsafe_allow_html=True)
    
    # Composant pour t√©l√©charger des donn√©es si n√©cessaire
    if st.session_state.data is None:
        upload_data_component()
    else:
        # Bouton pour changer de jeu de donn√©es
        if st.button("üì§ Changer de jeu de donn√©es"):
            upload_data_component()
    
    # Si des donn√©es sont charg√©es, afficher l'interface de recommandation
    if st.session_state.data is not None:
        df = st.session_state.data
        
        # V√©rifier si les coordonn√©es X et Y sont pr√©sentes
        if 'X' not in df.columns or 'Y' not in df.columns:
            st.error("Les colonnes X et Y sont n√©cessaires pour g√©n√©rer des recommandations de cibles.")
        else:
            # Informations sur le jeu de donn√©es
            st.markdown(f"<div class='info-card'>Jeu de donn√©es actuel: <strong>{st.session_state.uploaded_file_name}</strong> | {len(df)} √©chantillons | {len(df.columns)} variables</div>", unsafe_allow_html=True)
            
            # Configuration de la recommandation
            st.markdown("<h2 class='section-header'>‚öôÔ∏è Configuration des Recommandations</h2>", unsafe_allow_html=True)
            
            # Colonnes num√©riques (exclure X et Y)
            numeric_cols = [col for col in df.select_dtypes(include=['float64', 'int64']).columns if col not in ['X', 'Y']]
            
            col1, col2 = st.columns(2)
            
            with col1:
                # S√©lection des variables d'int√©r√™t
                target_cols = st.multiselect(
                    "S√©lectionner les variables d'int√©r√™t",
                    numeric_cols,
                    default=numeric_cols[:min(2, len(numeric_cols))]
                )
                
                # Nombre de recommandations
                num_reco = st.slider("Nombre de recommandations", 3, 50, 10)
            
            with col2:
                # M√©thode de recommandation
                method = st.selectbox(
                    "M√©thode de recommandation",
                    ["hybrid", "value", "exploration"],
                    format_func=lambda x: {
                        "hybrid": "Hybride (valeur + exploration)",
                        "value": "Zones de haute valeur",
                        "exploration": "Zones sous-√©chantillonn√©es"
                    }.get(x)
                )
                
                # Si hybride, ajouter le param√®tre de balance
                if method == "hybrid":
                    exploration_weight = st.slider(
                        "Balance exploration/exploitation",
                        0.0, 1.0, 0.5, 0.1,
                        help="0 = uniquement zones de haute valeur, 1 = uniquement zones sous-√©chantillonn√©es"
                    )
                else:
                    exploration_weight = 0.5  # Valeur par d√©faut
                
                # Distance minimale entre les recommandations
                min_distance = st.slider(
                    "Distance minimale entre recommandations (unit√©s)",
                    10, 1000, 100
                )
            
            # Param√®tres avanc√©s
            with st.expander("Param√®tres avanc√©s"):
                # Contraintes spatiales
                enable_constraints = st.checkbox("Ajouter des contraintes spatiales", value=False)
                
                if enable_constraints:
                    constraint_type = st.selectbox(
                        "Type de contrainte",
                        ["rectangle", "circle", "polygon"]
                    )
                    
                    if constraint_type == "rectangle":
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            x_min = st.number_input("X minimum", value=float(df['X'].min()))
                            y_min = st.number_input("Y minimum", value=float(df['Y'].min()))
                        
                        with col2:
                            x_max = st.number_input("X maximum", value=float(df['X'].max()))
                            y_max = st.number_input("Y maximum", value=float(df['Y'].max()))
                        
                        constraint_params = {
                            "type": "rectangle",
                            "x_min": x_min,
                            "y_min": y_min,
                            "x_max": x_max,
                            "y_max": y_max
                        }
                    
                    elif constraint_type == "circle":
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            center_x = st.number_input("Centre X", value=float(df['X'].mean()))
                            center_y = st.number_input("Centre Y", value=float(df['Y'].mean()))
                        
                        with col2:
                            radius = st.number_input("Rayon", value=float(min(df['X'].max() - df['X'].min(), df['Y'].max() - df['Y'].min()) / 4))
                        
                        constraint_params = {
                            "type": "circle",
                            "center_x": center_x,
                            "center_y": center_y,
                            "radius": radius
                        }
                    
                    elif constraint_type == "polygon":
                        st.info("Pour d√©finir un polygone, entrez les coordonn√©es des sommets (une paire X,Y par ligne).")
                        
                        polygon_coords = st.text_area(
                            "Coordonn√©es des sommets (format: X,Y)",
                            value=f"{df['X'].min()},{df['Y'].min()}\n{df['X'].max()},{df['Y'].min()}\n{df['X'].max()},{df['Y'].max()}\n{df['X'].min()},{df['Y'].max()}"
                        )
                        
                        # Parsing des coordonn√©es
                        try:
                            polygon_points = []
                            for line in polygon_coords.strip().split('\n'):
                                x, y = line.split(',')
                                polygon_points.append((float(x.strip()), float(y.strip())))
                            
                            constraint_params = {
                                "type": "polygon",
                                "points": polygon_points
                            }
                        except:
                            st.error("Format de coordonn√©es invalide. Utilisez le format 'X,Y' avec une paire par ligne.")
                            constraint_params = None
                else:
                    constraint_params = None
            
            # V√©rification des variables s√©lectionn√©es
            if not target_cols:
                st.warning("Veuillez s√©lectionner au moins une variable d'int√©r√™t.")
            else:
                # Bouton pour g√©n√©rer les recommandations
                gen_button = st.button("üéØ G√©n√©rer les recommandations")
                
                if gen_button:
                    # Affichage d'une barre de progression
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    
                    # G√©n√©ration des recommandations
                    status_text.text("G√©n√©ration des recommandations en cours...")
                    progress_bar.progress(30)
                    
                    # Appliquer les contraintes spatiales si n√©cessaire
                    if enable_constraints and constraint_params:
                        # Fonction pour v√©rifier si un point est dans la contrainte
                        def is_in_constraint(point, params):
                            x, y = point
                            
                            if params["type"] == "rectangle":
                                return (params["x_min"] <= x <= params["x_max"] and
                                        params["y_min"] <= y <= params["y_max"])
                            
                            elif params["type"] == "circle":
                                dx = x - params["center_x"]
                                dy = y - params["center_y"]
                                return dx**2 + dy**2 <= params["radius"]**2
                            
                            elif params["type"] == "polygon":
                                from matplotlib.path import Path
                                polygon_path = Path(params["points"])
                                return polygon_path.contains_point((x, y))
                            
                            return True  # Par d√©faut, pas de contrainte
                        
                        # Cr√©er une copie du dataframe pour appliquer les contraintes
                        constrained_df = df.copy()
                        
                        # Filtrer les points en dehors des contraintes
                        mask = constrained_df.apply(lambda row: is_in_constraint((row['X'], row['Y']), constraint_params), axis=1)
                        constrained_df = constrained_df[mask]
                        
                        if len(constrained_df) == 0:
                            st.error("Aucun point ne satisfait les contraintes spatiales. Veuillez ajuster les param√®tres.")
                            progress_bar.empty()
                            status_text.empty()
                        else:
                            # G√©n√©rer les recommandations avec le dataframe filtr√©
                            if method == "hybrid":
                                recommendations, grid_info = generate_sampling_recommendations(
                                    constrained_df, target_cols, num_reco, min_distance, method, exploration_weight
                                )
                            else:
                                recommendations, grid_info = generate_sampling_recommendations(
                                    constrained_df, target_cols, num_reco, min_distance, method
                                )
                            
                            progress_bar.progress(100)
                            progress_bar.empty()
                            status_text.empty()
                            
                            # Afficher les r√©sultats
                            if recommendations is not None:
                                st.success(f"‚úÖ {len(recommendations)} recommandations g√©n√©r√©es avec succ√®s!")
                                
                                # Afficher les r√©sultats
                                show_recommendation_results(recommendations, grid_info, constrained_df, target_cols)
                            else:
                                st.error("Une erreur s'est produite lors de la g√©n√©ration des recommandations.")
                    else:
                        # G√©n√©rer les recommandations sans contraintes
                        if method == "hybrid":
                            recommendations, grid_info = generate_sampling_recommendations(
                                df, target_cols, num_reco, min_distance, method, exploration_weight
                            )
                        else:
                            recommendations, grid_info = generate_sampling_recommendations(
                                df, target_cols, num_reco, min_distance, method
                            )
                        
                        progress_bar.progress(100)
                        progress_bar.empty()
                        status_text.empty()
                        
                        # Afficher les r√©sultats
                        if recommendations is not None:
                            st.success(f"‚úÖ {len(recommendations)} recommandations g√©n√©r√©es avec succ√®s!")
                            
                            # Afficher les r√©sultats
                            show_recommendation_results(recommendations, grid_info, df, target_cols)
                        else:
                            st.error("Une erreur s'est produite lors de la g√©n√©ration des recommandations.")

# Fonction pour afficher les r√©sultats des recommandations
def show_recommendation_results(recommendations, grid_info, df, target_cols):
    """Affiche les r√©sultats des recommandations de cibles."""
    st.markdown("<h2 class='section-header'>üìä R√©sultats</h2>", unsafe_allow_html=True)
    
    # Onglets pour diff√©rentes visualisations
    tab1, tab2, tab3 = st.tabs(["Carte des recommandations", "Tableau des r√©sultats", "Carte de chaleur d'int√©r√™t"])
    
    with tab1:
        st.markdown("<h3 class='subsection-header'>Carte des recommandations</h3>", unsafe_allow_html=True)
        
        # Pr√©parer les donn√©es pour la carte
        existing_df = df.copy()
        existing_df["Type"] = "Existant"
        
        reco_df = recommendations.copy()
        reco_df["Type"] = "Recommandation"
        
        # Fusionner les donn√©es pour la visualisation
        plot_data = pd.concat([
            existing_df[["X", "Y", "Type"]],
            reco_df[["X", "Y", "Type", "Rang"]]
        ])
        
        # Cr√©er la carte
        fig = px.scatter(
            plot_data,
            x="X", y="Y",
            color="Type",
            symbol="Type",
            color_discrete_map={"Existant": "#1E88E5", "Recommandation": "#FF5252"},
            symbol_map={"Existant": "circle", "Recommandation": "star"},
            title="Carte des recommandations de cibles",
            labels={"X": "Coordonn√©e X", "Y": "Coordonn√©e Y"}
        )
        
        # Ajouter des annotations pour les rangs des recommandations
        for _, row in reco_df.iterrows():
            fig.add_annotation(
                x=row["X"],
                y=row["Y"],
                text=str(int(row["Rang"])),
                showarrow=True,
                arrowhead=2,
                arrowsize=1,
                arrowwidth=2,
                arrowcolor="#FF5252",
                font=dict(size=12, color="white"),
                bgcolor="#FF5252",
                bordercolor="#FF5252",
                borderwidth=2,
                borderpad=4,
                ax=20,
                ay=-30
            )
        
        # Mise en page de la figure
        fig.update_layout(
            legend=dict(
                orientation="h",
                yanchor="bottom",
                y=1.02,
                xanchor="right",
                x=1
            )
        )
        
        # Afficher la carte
        st.plotly_chart(fig, use_container_width=True)
        
        # Afficher les distances entre les recommandations et les points existants
        with st.expander("Analyse des distances"):
            st.markdown("### Distances des recommandations aux points existants")
            
            # Calculer les distances minimales de chaque recommandation aux points existants
            existing_points = df[["X", "Y"]].values
            
            distance_data = []
            
            for idx, row in recommendations.iterrows():
                point = np.array([row["X"], row["Y"]])
                distances = np.sqrt(np.sum((existing_points - point)**2, axis=1))
                min_dist = np.min(distances)
                
                distance_data.append({
                    "Rang": int(row["Rang"]),
                    "X": row["X"],
                    "Y": row["Y"],
                    "Distance min.": min_dist,
                    "Point le plus proche": np.argmin(distances)
                })
            
            # Cr√©er un DataFrame avec les distances
            distance_df = pd.DataFrame(distance_data)
            
            # Afficher le tableau
            st.dataframe(distance_df.sort_values(by="Rang"), use_container_width=True)
            
            # Histogramme des distances
            fig = px.histogram(
                distance_df,
                x="Distance min.",
                title="Distribution des distances minimales aux points existants",
                color_discrete_sequence=["#FF7043"]
            )
            
            st.plotly_chart(fig, use_container_width=True)
    
    with tab2:
        st.markdown("<h3 class='subsection-header'>Tableau des recommandations</h3>", unsafe_allow_html=True)
        
        # Afficher le tableau des recommandations
        st.dataframe(recommendations, use_container_width=True)
        
        # T√©l√©chargement des r√©sultats
        csv = recommendations.to_csv(index=False)
        st.download_button(
            label="üì• T√©l√©charger les recommandations (CSV)",
            data=csv,
            file_name="recommandations_cibles.csv",
            mime="text/csv"
        )
        
        # Statistiques sur les valeurs estim√©es
        if len(target_cols) > 0:
            st.markdown("<h3 class='subsection-header'>Valeurs estim√©es</h3>", unsafe_allow_html=True)
            
            # Calculer des statistiques sur les valeurs estim√©es
            stats_data = []
            
            for col in target_cols:
                col_data = {
                    "Variable": col,
                    "Moyenne (existant)": df[col].mean(),
                    "M√©diane (existant)": df[col].median(),
                    "Min (existant)": df[col].min(),
                    "Max (existant)": df[col].max(),
                    "Moyenne (estim√©e)": recommendations[f"{col}_estim√©"].mean(),
                    "M√©diane (estim√©e)": recommendations[f"{col}_estim√©"].median(),
                    "Min (estim√©e)": recommendations[f"{col}_estim√©"].min(),
                    "Max (estim√©e)": recommendations[f"{col}_estim√©"].max()
                }
                
                stats_data.append(col_data)
            
            # Cr√©er un DataFrame avec les statistiques
            stats_df = pd.DataFrame(stats_data)
            
            # Afficher le tableau
            st.dataframe(stats_df, use_container_width=True)
            
            # Graphique comparatif des distributions
            comparison_data = []
            
            for col in target_cols:
                for val in df[col]:
                    comparison_data.append({
                        "Variable": col,
                        "Valeur": val,
                        "Type": "Existant"
                    })
                
                for val in recommendations[f"{col}_estim√©"]:
                    comparison_data.append({
                        "Variable": col,
                        "Valeur": val,
                        "Type": "Estim√©"
                    })
            
            comparison_df = pd.DataFrame(comparison_data)
            
            # Cr√©er les boxplots comparatifs
            fig = px.box(
                comparison_df,
                x="Variable",
                y="Valeur",
                color="Type",
                title="Comparaison des distributions: valeurs existantes vs estim√©es",
                color_discrete_map={"Existant": "#1E88E5", "Estim√©": "#FF7043"}
            )
            
            fig.update_layout(boxmode="group")
            
            st.plotly_chart(fig, use_container_width=True)
    
    with tab3:
        st.markdown("<h3 class='subsection-header'>Carte de chaleur d'int√©r√™t</h3>", unsafe_allow_html=True)
        
        # R√©cup√©rer les donn√©es de la grille pour la carte de chaleur
        grid_x = grid_info["grid_x"]
        grid_y = grid_info["grid_y"]
        grid_scores = grid_info["grid_scores"]
        
        # Cr√©er la carte de chaleur
        fig = go.Figure()
        
        # Ajouter la heatmap
        fig.add_trace(go.Heatmap(
            z=grid_scores,
            x=grid_x,
            y=grid_y,
            colorscale="Viridis",
            colorbar=dict(title="Score d'int√©r√™t")
        ))
        
        # Ajouter les points existants
        fig.add_trace(go.Scatter(
            x=df["X"],
            y=df["Y"],
            mode="markers",
            marker=dict(
                color="white",
                size=8,
                line=dict(
                    color="black",
                    width=1
                )
            ),
            name="Points existants"
        ))
        
        # Ajouter les recommandations
        fig.add_trace(go.Scatter(
            x=recommendations["X"],
            y=recommendations["Y"],
            mode="markers+text",
            marker=dict(
                color="red",
                size=10,
                symbol="star",
                line=dict(
                    color="white",
                    width=1
                )
            ),
            text=recommendations["Rang"],
            textposition="top center",
            name="Recommandations"
        ))
        
        # Mise en page
        fig.update_layout(
            title="Carte de chaleur d'int√©r√™t pour l'exploration",
            xaxis_title="Coordonn√©e X",
            yaxis_title="Coordonn√©e Y"
        )
        
        # Afficher la carte
        st.plotly_chart(fig, use_container_width=True)
        
        # Explication des scores
        with st.expander("Comprendre les scores d'int√©r√™t"):
            st.markdown("""
            ### Interpr√©tation des scores d'int√©r√™t
            
            Le score d'int√©r√™t est calcul√© en fonction de la m√©thode s√©lectionn√©e:
            
            - **Zones de haute valeur**: Le score est bas√© uniquement sur les valeurs interpol√©es des variables d'int√©r√™t. Les zones ayant des valeurs √©lev√©es ont des scores plus √©lev√©s.
            
            - **Zones sous-√©chantillonn√©es**: Le score est bas√© uniquement sur la distance aux points existants. Les zones √©loign√©es des points existants ont des scores plus √©lev√©s.
            
            - **Hybride**: Le score est une combinaison pond√©r√©e des deux approches pr√©c√©dentes, contr√¥l√©e par le param√®tre de balance exploration/exploitation.
            
            Les recommandations sont les points ayant les scores les plus √©lev√©s, tout en respectant la distance minimale entre elles.
            """)

# Lancement de l'application
if __name__ == "__main__":
    main()